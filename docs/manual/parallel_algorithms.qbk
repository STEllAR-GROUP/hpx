[/==============================================================================
    Copyright (C) 2007-2014 Hartmut Kaiser

    Distributed under the Boost Software License, Version 1.0. (See accompanying
    file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
===============================================================================/]

[section:parallel High Level Parallel Facilities]

In preparation for the upcoming C++ Standards we currently see several proposals
targeting different facilities supporting parallel programming. __hpx__
implements (and extends) some of those proposals. This is well aligned with
our strategy to align the APIs exposed from __hpx__ with current and future
C++ Standards.

At this point, __hpx__ implements several of the C++ Standardization working papers,
most notably __cpp11_n4104__ (Working Draft, Technical Specification for C++
Extensions for Parallelism), __cpp11_n4088__ (Task Blocks), and __cpp11_n4406__
(Parallel Algorithms Need Executors).


[section:parallel_algorithms Using Parallel Algorithms]

[def __sequential_execution_policy__ [classref hpx::parallel::execution::sequenced_policy `sequenced_policy`]]
[def __sequential_task_execution_policy__ [classref hpx::parallel::execution::sequenced_task_policy `sequenced_task_policy`]]
[def __parallel_execution_policy__ [classref hpx::parallel::execution::parallel_policy `parallel_policy`]]
[def __parallel_vector_execution_policy__ [classref hpx::parallel::execution::parallel_unsequenced_policy `parallel_unsequenced_policy`]]
[def __parallel_task_execution_policy__ [classref hpx::parallel::execution::parallel_task_policy `parallel_task_policy`]]
[def __execution_policy__ [classref hpx::parallel::v1::execution_policy `execution_policy`]]

[def __exception_list__ [classref hpx::exception_list `exception_list`]]

[def __par_for_each__ [funcref hpx::parallel::v1::for_each `for_each`]]

A parallel algorithm is a function template described by this document
which is declared in the (inline) namespace `hpx::parallel::v1`.

[note For compilers which do not support inline namespaces, all of the
      `namespace v1` is imported into the namespace `hpx::parallel`. The effect
      is similar to what inline namespaces would do, namely all names defined
      in `hpx::parallel::v1` are accessible from the namespace `hpx::parallel`
      as well.]

All parallel
algorithms are very similar in semantics to their sequential counterparts
(as defined in the `namespace std`) with an additional formal template parameter
named `ExecutionPolicy`. The execution policy is generally passed as the first
argument to any of the parallel algorithms and describes the manner in which
the execution of these algorithms may be parallelized and the manner in which
they apply user-provided function objects.

The applications of function objects in parallel algorithms invoked with an
execution policy object of type __sequential_execution_policy__ or
__sequential_task_execution_policy__ execute in sequential order. For
__sequential_execution_policy__ the execution happens in the calling thread.

The applications of function objects in parallel algorithms invoked with an
execution policy object of type __parallel_execution_policy__ or
__parallel_task_execution_policy__ are permitted to execute in an unordered fashion in
unspecified threads, and indeterminately sequenced within each thread.

[important It is the caller's responsibility to ensure correctness, for example
           that the invocation does not introduce data races or deadlocks.]

The applications of function objects in parallel algorithms invoked with an
execution policy of type __parallel_vector_execution_policy__ is in __hpx__
equivalent to the use of the execution policy __parallel_execution_policy__.

Algorithms invoked with an execution policy object of type __execution_policy__
execute internally as if invoked with the contained execution policy object.
No exception is thrown when an __execution_policy__ contains an execution policy
of type __sequential_task_execution_policy__ or __parallel_task_execution_policy__
(which normally turn the algorithm into its asynchronous version). In this case
the execution is semantically equivalent to the case of passing a
__sequential_execution_policy__ or __parallel_execution_policy__ contained in the
__execution_policy__ object respectively.

[heading Parallel Exceptions]

During the execution of a standard parallel algorithm, if temporary memory
resources are required by any of the algorithms and
no memory are available, the algorithm throws a `std::bad_alloc` exception.

During the execution of any of the parallel algorithms, if the application
of a function object terminates with an uncaught exception, the behavior
of the program is determined by the type of execution policy used to invoke
the algorithm:

* If the execution policy object is of type __parallel_vector_execution_policy__,
  [funcref hpx::terminate] shall be called.

* If the execution policy object is of type __sequential_execution_policy__,
  __sequential_task_execution_policy__, __parallel_execution_policy__, or
  __parallel_task_execution_policy__ the execution of
  the algorithm terminates with an __exception_list__ exception. All
  uncaught exceptions thrown during the application of user-provided
  function objects shall be contained in the __exception_list__.

For example, the number of invocations of the user-provided
function object in for_each is unspecified. When __par_for_each__ is
executed sequentially, only one exception will be contained in
the __exception_list__ object.

These guarantees imply that, unless the algorithm has failed to
allocate memory and terminated with `std::bad_alloc`, all
exceptions thrown during the execution of the algorithm are
communicated to the caller. It is unspecified whether an algorithm
implementation will "forge ahead" after encountering and capturing
a user exception.

The algorithm may terminate with the `std::bad_alloc` exception
even if one or more user-provided function objects have terminated
with an exception. For example, this can happen when an algorithm
fails to allocate memory while creating or adding elements to the
__exception_list__ object.

[heading Parallel Algorithms]

__hpx__ provides implementations of the following parallel algorithms:

[table Non-modifying Parallel Algorithms (In Header: <hpx/include/parallel_algorithm.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algoref all_of] ]
     [Checks if a predicate is `true` for all  of the elements in a range.]
     [`<hpx/include/parallel_all_any_none.hpp>`]]
    [[ [algoref any_of] ]
     [Checks if a predicate is `true` for any of the elements in a range.]
     [`<hpx/include/parallel_all_any_none.hpp>`]]
    [[ [algoref none_of] ]
     [Checks if a predicate is `true` for none of the elements in a range.]
     [`<hpx/include/parallel_all_any_none.hpp>`]]
    [[ [algoref for_each] ]
     [Applies a function to a range of elements.]
     [`<hpx/include/parallel_for_each.hpp>`]]
    [[ [algoref for_each_n] ]
     [Applies a function to a number of elements.]
     [`<hpx/include/parallel_for_each.hpp>`]]
    [[ [algoref count] ]
     [Returns the number of elements equal to a given value.]
     [`<hpx/include/parallel_count.hpp>`]]
    [[ [algoref count_if] ]
     [Returns the number of elements satisfying a specific criteria.]
     [`<hpx/include/parallel_count.hpp>`]]
    [[ [algoref equal] ]
     [Determines if two sets of elements are the same.]
     [`<hpx/include/parallel_equal.hpp>`]]
    [[ [algoref mismatch] ]
     [Finds the first position where two ranges differ.]
     [`<hpx/include/parallel_mismatch.hpp>`]]
    [[ [algoref find] ]
     [Finds the first element equal to a given value.]
     [`<hpx/include/parallel_find.hpp>`]]
    [[ [algoref find_end] ]
     [Finds the last sequence of elements in a certain range.]
     [`<hpx/include/parallel_find.hpp>`]]
    [[ [algoref find_if] ]
     [Finds the first element satisfying a specific criteria.]
     [`<hpx/include/parallel_find.hpp>`]]
    [[ [algoref find_first_of] ]
     [Searches for any one of a set of elements.]
     [`<hpx/include/parallel_find.hpp>`]]
    [[ [algoref find_if_not] ]
     [Finds the first element not satisfying a specific criteria.]
     [`<hpx/include/parallel_find.hpp>`]]
    [[ [algoref adjacent_find] ]
     [Computes the differences between adjacent elements in a range.]
     [`<hpx/include/parallel_adjacent_find.hpp>`]]
    [[ [algoref lexicographical_compare] ]
     [Checks if a range of values is lexicographically less than another range
      of values.]
     [`<hpx/include/parallel_lexicographical_compare.hpp>`]]
    [[ [algoref search] ]
     [Searches for a range of elements.]
     [`<hpx/include/parallel_search.hpp>`]]
    [[ [algoref search_n] ]
     [Searches for a number consecutive copies of an element in a range.]
     [`<hpx/include/parallel_search.hpp>`]]
    [[ [algoref inclusive_scan] ]
     [Does an inclusive parallel scan over a range of elements.]
     [`<hpx/include/parallel_scan.hpp>`]]
    [[ [algoref exclusive_scan] ]
     [Does an exclusive parallel scan over a range of elements.]
     [`<hpx/include/parallel_scan.hpp>`]]
]

[table Modifying Parallel Algorithms (In Header: <hpx/include/parallel_algortithm.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algoref copy] ]
     [Copies a range of elements to a new location.]
     [`<hpx/include/parallel_copy.hpp>`]]
    [[ [algoref copy_n] ]
     [Copies a number of elements to a new location.]
     [`<hpx/include/parallel_copy.hpp>`]]
    [[ [algoref copy_if] ]
     [Copies the elements from a range to a new location for which the given
      predicate is `true`.]
     [`<hpx/include/parallel_copy.hpp>`]]
    [[ [algoref move] ]
     [Moves a range of elements to a new location.]
     [`<hpx/include/parallel_fill.hpp>`]]
    [[ [algoref fill] ]
     [Assigns a range of elements a certain value.]
     [`<hpx/include/parallel_fill.hpp>`]]
    [[ [algoref fill_n] ]
     [Assigns a value to a number of elements.]
     [`<hpx/include/parallel_fill.hpp>`]]
    [[ [algoref transform] ]
     [Applies a function to a range of elements.]
     [`<hpx/include/parallel_transform.hpp>`]]
    [[ [algoref generate] ]
     [Saves the result of a function in a range.]
     [`<hpx/include/parallel_generate.hpp>`]]
    [[ [algoref generate_n] ]
     [Saves the result of N applications of a function.]
     [`<hpx/include/parallel_generate.hpp>`]]
    [[ [algoref remove_copy] ]
     [Copies the elements from a range to a new location that are not equal to
      the given value.]
     [`<hpx/include/parallel_remove_copy.hpp>`]]
    [[ [algoref remove_copy_if] ]
     [Copies the elements from a range to a new location for which the given
      predicate is `false`.]
     [`<hpx/include/parallel_remove_copy.hpp>`]]
    [[ [algoref replace] ]
     [Replaces all values satisfying specific criteria with another value.]
     [`<hpx/include/parallel_replace.hpp>`]]
    [[ [algoref replace_if] ]
     [Replaces all values satisfying specific criteria with another value.]
     [`<hpx/include/parallel_replace.hpp>`]]
    [[ [algoref replace_copy] ]
     [Copies a range, replacing elements satisfying specific criteria with
      another value.]
     [`<hpx/include/parallel_replace.hpp>`]]
    [[ [algoref replace_copy_if] ]
     [Copies a range, replacing elements satisfying specific criteria with
      another value.]
     [`<hpx/include/parallel_replace.hpp>`]]
    [[ [algoref reverse] ]
     [Reverses the order elements in a range.]
     [`<hpx/include/parallel_reverse.hpp>`]]
    [[ [algoref reverse_copy] ]
     [Creates a copy of a range that is reversed.]
     [`<hpx/include/parallel_reverse.hpp>`]]
    [[ [algoref rotate] ]
     [Rotates the order of elements in a range.]
     [`<hpx/include/parallel_rotate.hpp>`]]
    [[ [algoref rotate_copy] ]
     [Copies and rotates a range of elements.]
     [`<hpx/include/parallel_rotate.hpp>`]]
    [[ [algoref swap_ranges] ]
     [Swaps two ranges of elements.]
     [`<hpx/include/parallel_swap_ranges.hpp>`]]
]

[table Set operations on sorted sequences(In Header: <hpx/include/parallel_algortithm.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algoref includes] ]
     [Returns true if one set is a subset of another.]
     [`<hpx/include/parallel_set_operations.hpp>`]]
    [[ [algoref set_difference] ]
     [Computes the difference between two sets.]
     [`<hpx/include/parallel_set_operations.hpp>`]]
    [[ [algoref set_intersection] ]
     [Computes the intersection of two sets.]
     [`<hpx/include/parallel_set_operations.hpp>`]]
    [[ [algoref set_symmetric_difference] ]
     [Computes the symmetric difference between two sets.]
     [`<hpx/include/parallel_set_operations.hpp>`]]
    [[ [algoref set_union] ]
     [Computes the union of two sets.]
     [`<hpx/include/parallel_set_operations.hpp>`]]
]

[table Minimum/maximum operations (In Header: <hpx/include/parallel_algortithm.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algoref max_element] ]
     [Returns the largest element in a range.]
     [`<hpx/include/parallel_minmax.hpp>`]]
    [[ [algoref min_element] ]
     [Returns the smallest element in a range.]
     [`<hpx/include/parallel_minmax.hpp>`]]
    [[ [algoref minmax_element] ]
     [Returns the smallest and the largest element in a range.]
     [`<hpx/include/parallel_minmax.hpp>`]]
]

[table Sorting Operations (In Header: <hpx/include/parallel_algorithm.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algoref is_sorted] ]
     [Returns `true` if each element in a range is sorted]
     [`<hpx/include/parallel_is_sorted.hpp>`]]
    [[ [algoref is_sorted_until] ]
     [Returns the first unsorted element]
     [`<hpx/include/parallel_is_sorted.hpp>`]]
    [[ [algoref is_partitioned] ]
     [Returns `true` if each true element for a predicate precedes the false elements in a range]
     [`<hpx/include/parallel_is_partitioned.hpp>`]]
    [[ [algoref sort] ]
     [Sorts the elements in a range]
     [`<hpx/include/parallel_sort.hpp>`]]
    [[ [algoref sort_by_key] ]
     [Sorts one range of data using keys supplied in another range]
     [`<hpx/include/parallel_sort.hpp>`]]
]

[table Numeric Parallel Algorithms (In Header: <hpx/include/parallel_numeric.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algoref adjacent_difference] ]
     [Calculates the difference between each element in an input range and the preceding element.]
     [`<hpx/include/parallel_adjacent_difference.hpp>`]]
    [[ [algoref reduce] ]
     [Sums up a range of elements.]
     [`<hpx/include/parallel_reduce.hpp>`]]
    [[ [algoref reduce_by_key] ]
     [Performs an inclusive scan on consecutive elements with matching keys, with a reduction
      to output only the final sum for each key. The key sequence {1,1,1,2,3,3,3,3,1} and value
      sequence {2,3,4,5,6,7,8,9,10} would be reduced to keys={1,2,3,1}, values={9,5,30,10}]
     [`<hpx/include/parallel_reduce.hpp>`]]
    [[ [algoref transform_reduce] ]
     [Sums up a range of elements after applying a function. Also, accumulates
      the inner products of two input ranges.]
     [`<hpx/include/parallel_transform_reduce.hpp>`]]
    [[ [algoref transform_inclusive_scan] ]
     [Does an inclusive parallel scan over a range of elements after applying
      a function.]
     [`<hpx/include/parallel_scan.hpp>`]]
    [[ [algoref transform_exclusive_scan] ]
     [Does an exclusive parallel scan over a range of elements after applying
      a function.]
     [`<hpx/include/parallel_scan.hpp>`]]
]

[table Dynamic Memory Management (In Header: <hpx/include/parallel_memory.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algoref uninitialized_copy] ]
     [Copies a range of objects to an uninitialized area of memory.]
     [`<hpx/include/parallel_uninitialized_copy.hpp>`]]
    [[ [algoref uninitialized_copy_n] ]
     [Copies a number of objects to an uninitialized area of memory.]
     [`<hpx/include/parallel_uninitialized_copy.hpp>`]]
    [[ [algoref uninitialized_fill] ]
     [Copies an object to an uninitialized area of memory.]
     [`<hpx/include/parallel_uninitialized_fill.hpp>`]]
    [[ [algoref uninitialized_fill_n] ]
     [Copies an object to an uninitialized area of memory.]
     [`<hpx/include/parallel_uninitialized_fill.hpp>`]]
]

[table Index-based for-loops (In Header: <hpx/include/parallel_algorithm.hpp>)
    [[Name]     [Description]   [In Header]]
    [[ [algorefv2 for_loop] ]
     [Implements loop functionality over a range specified by integral or iterator bounds.]
     [`<hpx/include/parallel_for_loop.hpp>`]]
    [[ [algorefv2 for_loop_strided] ]
     [Implements loop functionality over a range specified by integral or iterator bounds.]
     [`<hpx/include/parallel_for_loop.hpp>`]]
    [[ [algorefv2 for_loop_n] ]
     [Implements loop functionality over a range specified by integral or iterator bounds.]
     [`<hpx/include/parallel_for_loop.hpp>`]]
    [[ [algorefv2 for_loop_n_strided] ]
     [Implements loop functionality over a range specified by integral or iterator bounds.]
     [`<hpx/include/parallel_for_loop.hpp>`]]
]

[endsect]

[//////////////////////////////////////////////////////////////////////////////]
[section:executors Executors and Executor Traits]

The existing Version 1 of the Parallelism TS (__cpp11_n4104__) exposes parallel
execution to the programmer in the form of standard algorithms that accept
execution policies. A companion executor facility both provides a suitable
substrate for implementing these algorithms in a standard way and provide a
mechanism for exercising programmatic control over where parallel work should
be executed.

The algorithms and execution policies specified by the Parallelism TS are
designed to permit implementation on the broadest range of platforms. In
addition to preemptive thread pools common on some platforms, implementations
of these algorithms may want to take advantage of a number of mechanisms for
parallel execution, including cooperative fibers, GPU threads, and SIMD vector
units, among others. This diversity of possible execution resources strongly
suggests that a suitable abstraction encapsulating the details of how
work is created across diverse platforms would be of significant value to
parallel algorithm implementations. Suitably defined executors provide just
such a facility.

An executor is an object responsible for creating execution agents on which
work is performed, thus abstracting the (potentially platform-specific)
mechanisms for launching work. To accommodate the goals of the Parallelism TS,
whose algorithms aim to support the broadest range of possible platforms, the
requirements that all executors are expected to fulfill are small. They are
also consistent with a broad range of execution semantics, including
preemptive threads, cooperative fibers, GPU threads, and SIMD vector
units, among others.

The executors implemented by __hpx__ are aligned with the interfaces proposed
by __cpp11_n4406__ (Parallel Algorithms Need Executors).

Executors are modular components for requisitioning execution agents. During
parallel algorithm execution, execution policies generate execution agents by
requesting their creation from an associated executor. Rather than focusing on
asynchronous task queueing, our complementary treatment of executors casts
them as modular components for invoking functions over the points of an index
space. We believe that executors may be conceived of as allocators for
execution agents and our interface's design reflects this analogy. The process
of requesting agents from an executor is mediated via the
[classref hpx::parallel::v3::executor_traits `hpx::parallel::executor_traits`]
API, which is analogous to the interaction between containers and
allocator_traits.

With `executor_traits`, clients manipulate all types of executors uniformly:

    executor_traits<my_executor_t>::execute(my_executor,
        [](size_t i){ // perform task i },
        range(0, n));

This call synchronously creates a group of invocations of the given function,
where each individual invocation within the group is identified by a unique
integer i in `[0, n)`. Other functions in the interface exist to create groups of
invocations asynchronously and support the special case of creating a singleton
group, resulting in four different combinations.

Though this interface appears to require executor authors to implement four
different basic operations, there is really only one requirement:
`async_execute()`. In practice, the other operations may be defined in terms of
this single basic primitive. However, some executors will naturally specialize
all four operations for maximum efficiency.

For maximum implementation flexibility, `executor_traits` does not require
executors to implement a particular exception reporting mechanism. Executors
may choose whether or not to report exceptions, and if so, in what manner they
are communicated back to the caller. However, all executors in __hpx__
report exceptions in a manner consistent with the behavior of execution policies
described by the Parallelism TS, where multiple exceptions are collected into
an __exception_list__. This list is reported through `async_execute()`'s
returned future, or thrown directly by execute().

[important Please note that the executor interface as described above has no been
           deprecated. It has been replaced by separate executor customization
           points, one for each of the exposed functions. Please see __cpp20_p0443__
           for more details. The old interface based on `executor_traits` will
           be supported for some time, you might have to enable the
           `HPX_WITH_EXECUTOR_COMPATIBILITY` configuration setting, however.]

In __hpx__ we have implemented the following executor types:

* [classref hpx::parallel::execution::sequenced_executor `hpx::parallel::execution::sequenced_executor`]:
  creates groups of sequential execution agents which execute in the calling
  thread. The sequential order is given by the lexicographical order of indices
  in the index space.
* [classref hpx::parallel::execution::parallel_executor `hpx::parallel::execution::parallel_executor`]:
  creates groups of parallel execution agents which execute in threads
  implicitly created by the executor. This executor uses a given launch policy.
* [classref hpx::parallel::execution::service_executor `hpx::parallel::execution::service_executor`]:
  creates groups of parallel execution agents which execute in one of the
  kernel threads associated with a given pool category (I/O, parcel, or timer
  pool, or on the main thread of the application).
* [classref hpx::parallel::execution::local_priority_queue_executor `hpx::parallel::execution::local_priority_queue_executor`],
  [classref hpx::parallel::execution::local_queue_executor `hpx::parallel::execution::local_queue_executor`]
  [classref hpx::parallel::execution::static_priority_queue_executor `hpx::parallel::execution::static_priority_queue_executor`]
  create executors on top of the corresponding __hpx__ schedulers.
* [classref hpx::parallel::execution::distribution_policy_executor `hpx::parallel::execution::distribution_policy_executor`]
  creates executors using any of the existing distribution policies (like
  [classref hpx::components::colocating_distribution_policy `hpx::components::colocating_distribution_policy]
  et.al.).

[endsect]

[//////////////////////////////////////////////////////////////////////////////]
[section:executor_parameters Executor Parameters and Executor Parameter Traits]

Executors as described in the previous section add a powerful customization
capability to any facility which exposes management of parallel execution.
However, sometimes it is necessary to be able to customize certain parameters
of the execution as well. In __hpx__ we introduce the notion of execution
parameters and execution parameter traits. At this point, the only parameter
which can be customized is the size of the chunks of work executed on a single
__hpx__-thread (such as the number of loop iterations combined to run as a
single task).

An executor parameter object is responsible for exposing the calculation of the
size of the chunks scheduled. It abstracts the (potential platform-specific)
algorithms of determining those chunks sizes.

The way executor parameters are implemented is aligned with the way executors
are implemented. All functionalities of concrete executor parameter types are
exposed and accessible through a corresponding
[classref hpx::parallel::v3::executor_parameter_traits `hpx::parallel::executor_parameter_traits`]
type.

With `executor_parameter_traits,` clients access all types of executor
parameters uniformly:

    std::size_t chunk_size =
        executor_parameter_traits<my_parameter_t>::get_chunk_size(my_parameter,
            my_executor, [](){ return 0; }, num_tasks);

This call synchronously retrieves the size of a single chunk of loop iterations
(or similar) to combine for execution on a single __hpx__-thread if the overall
number of tasks to schedule is given by `num_tasks`. The lambda
function exposes a means of test-probing the execution of a single iteration
for performance measurement purposes (the execution parameter type might
dynamically determine the execution time of one or more tasks in order to
calculate the chunk size, see
[classref hpx::parallel::v3::auto_chunk_size `hpx::parallel::auto_chunk_size`]
for an example of such a executor parameter type).

Other functions in the interface exist to discover whether a executor parameter
type should be invoked once (i.e. returns a static chunk size, see
[classref hpx::parallel::v3::static_chunk_size `hpx::parallel::static_chunk_size`])
or whether it should be invoked for each scheduled chunk of work (i.e. it
returns a variable chunk size, for an example, see
[classref hpx::parallel::v3::guided_chunk_size `hpx::parallel::guided_chunk_size`]).

Though this interface appears to require executor parameter type authors to
implement all different basic operations, there is really none required. In
practice, all operations have sensible defaults. However, some executor
parameter types will naturally specialize all operations for maximum efficiency.

In __hpx__ we have implemented the following executor parameter types:

* [classref hpx::parallel::v3::auto_chunk_size `hpx::parallel::auto_chunk_size`]:
  Loop iterations are divided into pieces and then assigned to threads. The
  number of loop iterations combined is determined based on measurements of how
  long the execution of 1% of the overall number of iterations takes.
  This executor parameters type makes sure that as many loop iterations
  are combined as necessary to run for the amount of time specified.
* [classref hpx::parallel::v3::static_chunk_size `hpx::parallel::static_chunk_size`]:
  Loop iterations are divided into pieces of a given size and then assigned to
  threads. If the size is not specified, the iterations are evenly (if
  possible) divided contiguously among the threads. This executor parameters
  type is equivalent to OpenMP's STATIC scheduling directive.
* [classref hpx::parallel::v3::dynamic_chunk_size `hpx::parallel::dynamic_chunk_size`]:
  Loop iterations are divided into pieces of a given size and then dynamically
  scheduled among the cores; when an core finishes one chunk, it is dynamically
  assigned another If the size is not specified, the default chunk size is 1.
  This executor parameters type is equivalent to OpenMP's DYNAMIC scheduling
  directive.
* [classref hpx::parallel::v3::guided_chunk_size `hpx::parallel::guided_chunk_size`]:
  Iterations are dynamically assigned to cores in blocks as cores request them
  until no blocks remain to be assigned. Similar to `dynamic_chunk_size` except
  that the block size decreases each time a number of loop iterations is given
  to a thread. The size of the initial block is proportional to
  `number_of_iterations / number_of_cores`.
  Subsequent blocks are proportional to
  `number_of_iterations_remaining / number_of_cores`. The optional chunk size
  parameter defines the minimum block size. The default minimal chunk size is 1.
  This executor parameters type is equivalent to OpenMP's GUIDED scheduling
  directive.

[endsect]

[//////////////////////////////////////////////////////////////////////////////]
[section:task_block Using Task Blocks]

The `define_task_block`, `run` and the `wait` functions implemented based on
__cpp11_n4088__ are based on the task_block concept that is a part of the
common subset of the __ppl__ and the __tbb__ libraries.

This implementations adopts a simpler syntax than exposed by those libraries -
one that is influenced by language-based concepts such as spawn and sync from
__cilk_pp__ and async and finish from __x10__. It improves on existing practice
in the following ways:

* The exception handling model is simplified and more consistent with normal
  C++ exceptions.
* Most violations of strict fork-join parallelism can be enforced at compile
  time (with compiler assistance, in some cases).
* The syntax allows scheduling approaches other than child stealing.

Consider an example of a parallel traversal of a tree, where a user-provided
function compute is applied to each node of the tree, returning the sum of the
results:

    template <typename Func>
    int traverse(node& n, Func && compute)
    {
        int left = 0, right = 0;
        define_task_block(
            [&](task_block<>& tr) {
                if (n.left)
                    tr.run([&] { left = traverse(*n.left, compute); });
                if (n.right)
                    tr.run([&] { right = traverse(*n.right, compute); });
            });

        return compute(n) + left + right;
    }

The example above demonstrates the use of two of the functions,
[funcref hpx::parallel::v2::define_task_block define_task_block] and the
[memberref hpx::parallel::v2::task_block::run run] member
function of a [classref hpx::parallel::v2::task_block task_block].

The `task_block` function delineates a region in a program code potentially
containing invocations of threads spawned by the `run` member function of the
`task_block` class.
The `run` function spawns an __hpx__ thread, a unit of work that is allowed to
execute in parallel with respect to the caller. Any parallel tasks spawned by
`run` within the task block are joined back to a single thread of execution
at the end of the `define_task_block`.
`run` takes a user-provided function object `f` and starts it asynchronously -
i.e. it may return before the execution of `f` completes. The __hpx__
scheduler may choose to run `f` immediately or delay running `f` until compute
resources become available.

A `task_block` can be constructed only by `define_task_block` because it has
no public constructors.
Thus, `run` can be invoked (directly or indirectly) only from a user-provided
function passed to `define_task_block`:

    void g();

    void f(task_block<>& tr)
    {
        tr.run(g);          // OK, invoked from within task_block in h
    }

    void h()
    {
        define_task_block(f);
    }

    int main()
    {
        task_block<> tr;    // Error: no public constructor
        tr.run(g);          // No way to call run outside of a define_task_block
        return 0;
    }

[endsect]

[section:task_block Extensions for Task Blocks]

[heading Using Execution Policies with Task Blocks]

In __hpx__ we implemented some extensions for `task_block` beyond the actual
standards proposal __cpp11_n4088__. The main addition is that a `task_block`
can be invoked with a execution policy as its first argument, very similar to
the parallel algorithms.

An execution policy is an object that expresses the requirements on the
ordering of functions invoked as a consequence of the invocation of a
task block. Enabling passing an execution policy to `define_task_block`
gives the user control over the amount of parallelism employed by the
created `task_block`. In the following example the use of an explicit
`par` execution policy makes the user's intent explicit:

    template <typename Func>
    int traverse(node *n, Func&& compute)
    {
        int left = 0, right = 0;

        define_task_block(
            execution::par,                // execution::parallel_policy
            [&](task_block<>& tb) {
                if (n->left)
                    tb.run([&] { left = traverse(n->left, compute); });
                if (n->right)
                    tb.run([&] { right = traverse(n->right, compute); });
            });

        return compute(n) + left + right;
    }

This also causes the [classref hpx::parallel::v2::task_block `hpx::parallel::task_block`]
object to be a template in our implementation.
The template argument is the type of the execution policy used to create the
task block. The template argument defaults to
[classref hpx::parallel::execution::parallel_policy `hpx::parallel::execution::parallel_policy`].

__hpx__ still supports calling
[funcref hpx::parallel::v2::define_task_block `hpx::parallel::define_task_block`]
without an explicit execution policy. In this case the task block will run using
the [classref hpx::parallel::execution::parallel_policy `hpx::parallel::execution::parallel_policy`].

__hpx__ also adds the ability to access the execution policy which was used to
create a given `task_block`.

[heading Using Executors to run Tasks]

Often, we want to be able to not only define an execution policy to use by
default for all spawned tasks inside the task block, but in addition to
customize the execution context for one of the tasks executed by
`task_block::run`. Adding an optionally passed executor instance to that
function enables this use case:

    template <typename Func>
    int traverse(node *n, Func&& compute)
    {
        int left = 0, right = 0;

        define_task_block(
            execution::par,                // execution::parallel_policy
            [&](auto& tb) {
                if (n->left)
                {
                    // use explicitly specified executor to run this task
                    tb.run(my_executor(), [&] { left = traverse(n->left, compute); });
                }
                if (n->right)
                {
                    // use the executor associated with the par execution policy
                    tb.run([&] { right = traverse(n->right, compute); });
                }
            });

        return compute(n) + left + right;
    }

__hpx__ still supports calling
[funcref hpx::parallel::v2::task_block::run `hpx::parallel::task_block::run`]
without an explicit executor object. In this case the task will be run using
the executor associated with the execution policy which was used to call
[funcref hpx::parallel::v2::define_task_block `hpx::parallel::define_task_block`].

[endsect]

[endsect]
