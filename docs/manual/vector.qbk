[/==============================================================================
    Copyright (C) 2017 Antoine Tran Tan

    Distributed under the Boost Software License, Version 1.0. (See accompanying
    file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
===============================================================================/]

[section:containers Segmented Containers]

In parallel programming, there is now a plethora of solutions aimed at
implementing "partially contiguous" or segmented data structures,
whether on shared memory systems or distributed memory systems. __hpx__
implements such structures by drawing inspiration from Standard C++ containers.

[section:parallel_containers Using Segmented Containers]

[def __container_layout__
[classref hpx::container_distribution_policy `container_distribution_policy`]]

A segmented container is a template class that is described in the namespace
`hpx`. All segmented containers are very similar semantically to their
sequential counterpart (defined in `namespace std`) but with an additional
template parameter named `DistPolicy`. The distribution policy is an
optional parameter that is passed last to the segmented container constructor
(after the container size when no default value is given, after the default value
if not). The distribution policy describes the manner in which a container is
segmented and the placement of each segment among the available runtime
localities.

However, only a part of the `std` container member functions were reimplemented:

* [def Object based member functions]
  `(constructor)`, `(destructor)`, `operator=`
* [def Element access]
  `operator[]`
* [def Iterators]
  `begin`, `cbegin`, `end`, `cend`
* [def Capacity]
  `size`

An example of how to use the `partitioned_vector` container
would be :

    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(double);

    // By default, the number of segments is equal to the current number of
    // localities
    //
    hpx::partitioned_vector<double> va(50);
    hpx::partitioned_vector<double> vb(50, 0.0);


An example of how to use the `partitioned_vector` container
with distribution policies would be:

    #include <hpx/include/partitioned_vector.hpp>
    #include <hpx/runtime/find_localities.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(double);

    std::size_t num_segments = 10;
    std::vector<hpx::id_type> locs = hpx::find_all_localities()

    auto layout =
            hpx::container_layout( num_segments, locs );

    // The number of segments is 10 and those segments are spread across the
    // localities collected in the variable locs in a Round-Robin manner
    //
    hpx::partitioned_vector<double> va(50, layout);
    hpx::partitioned_vector<double> vb(50, 0.0, layout);

By definition, a segmented container must be accessible from
any thread although its construction is synchronous only for the thread who has
called its constructor. To overcome this problem, it is possible to assign a
symbolic name to the segmented container.

    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(double);

    hpx::future<void> fserver = hpx::async(
      [](){
        hpx::partitioned_vector<double> v(50);

        // Register the 'partitioned_vector' with the name "some_name"
        //
        v.register_as("some_name");

        /* Do some code  */
      });

    hpx::future<void> fclient =
      hpx::async(
        [](){
          // Naked 'partitioned_vector'
          //
          hpx::partitioned_vector<double> v;

          // Now the variable v points to the same 'partitioned_vector' that has
          // been registered with the name "some_name"
          //
          v.connect_to("some_name");

          /* Do some code  */
        });

[heading Segmented Containers]

__hpx__ provides the following segmented containers:

[table Sequence Containers
    [[Name]     [Description]   [In Header] [Class page at cppreference.com]]
    [[ [hpx::partitioned_vector] ]
     [Dynamic segmented contiguous array.]
     [`<hpx/include/partitioned_vector.hpp>`]
     [[cpprefcontdocs vector]]
    ]
]

[table Unordered Associative Containers
    [[Name]     [Description]   [In Header] [Class page at cppreference.com]]
    [[ [hpx::unordered_map] ]
     [Segmented collection of key-value pairs, hashed by keys, keys are unique.]
     [`<hpx/include/unordered_map.hpp>`]
     [[cpprefcontdocs unordered_map]]
    ]
]

[endsect]

[//////////////////////////////////////////////////////////////////////////////]
[section:segmented_iterators Segmented Iterators and Segmented Iterator Traits]

The basic iterator used in the STL library is only suitable for
one-dimensional structures. The iterators we use in `hpx` must adapt to
the segmented format of our containers. Our iterators are then able to know when
incrementing themselves if the next element of type `T` is in the same data
segment or in another segment. In this second case, the iterator will
automatically point to the beginning of the next segment.

[note Note that the dereference operation (`operator *`) does not directly return
      a reference of type `T&` but an intermediate object wrapping this
      reference. When this object is used as an l-value, a remote write
      operation is performed; When this object is used as an r-value,
      implicit conversion to `T` type will take care of performing remote read
      operation.
]

It is sometimes useful not only to iterate element by element, but also
segment by segment, or simply get a local iterator in order to avoid
additional construction costs at each deferencing operations. To mitigate
this need, the [classref hpx::traits::segmented_iterator_traits
`hpx::traits::segmented_iterator_traits`] are used.

With `segmented_iterator_traits`, users can uniformly get the iterators
which specifically iterates over segments (by providing a segmented iterator
as a parameter), or get the local begin/end iterators of the nearest
local segment (by providing a per-segment iterator as a parameter).

    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(double);

    using iterator = hpx::partitioned_vector<T>::iterator;
    using traits   = hpx::traits::segmented_iterator_traits<iterator>;

    hpx::partitioned_vector<T> v;
    std::size_t count = 0;

    auto seg_begin = traits::segment(v.begin());
    auto seg_end   = traits::segment(v.end());

    // Iterate over segments
    for (auto seg_it = seg_begin; seg_it != seg_end; ++seg_it)
    {
        auto loc_begin = traits::begin(seg_it)
        auto loc_end   = traits::end(seg_it);

        // Iterate over elements inside segments
        for (auto lit = loc_begin; lit != loc_end; ++lit, ++count)
        {
            *lit = count;
        }
    }

Which is equivalent to:

    hpx::partitioned_vector<T> v;
    std::size_t count = 0;

    auto begin = v.begin();
    auto end   = v.end();

    for (auto it = begin; it != end; ++it, ++count)
    {
        *it = count;
    }

[endsect]

[//////////////////////////////////////////////////////////////////////////////]
[section:partitioned_vector_views Using Views]

The use of multi-dimensional arrays is quite common
in the numerical field whether to perform dense matrix operations
or to process images. It exist many libraries which implement such object
classes overloading their basic operators (e.g. `+`, `-`, `*`, `()`, etc.).
However, such operation becomes more delicate when the underlying data layout
is segmented or when it is mandatory to use fully-optimimized linear algebra
subroutines (i.e. BLAS subroutines).

Our solution is thus to relax the level of abstraction by allowing
the user to work not directly on n-dimensionnal data, but on "n-dimensionnal
collections of 1-D arrays". The use of well-accepted techniques on contiguous
data is thus preserved at the segment level, and the composability of the
segments is made possible thanks to multi-dimensional array -inspired
access mode.

[section:spmd_block Preface : Why SPMD ?]

Although HPX refutes by design this programming model, the locality plays a
dominant role when it comes to implement vectorized code. To maximize local
computing and avoid uneededed data transfer, a parallel section (or Single
Programming Multiple Data section) is required. Because the use of global
variables is prohibited, this parallel section is created via the RAII idiom.

To define a parallel section, simply write an action taking a `spmd_block`
variable as the first parameter.

    #include <hpx/lcos/spmd_block.hpp>

    void bulk_function(hpx::lcos::spmd_block block /* , arg0, arg1, ... */)
    {
        // Parallel section

        /* Do some code */
    }
    HPX_PLAIN_ACTION(bulk_function, bulk_action);

[note In the following paragraphs, we will use the term "image" several times.
      An image is defined as a lightweight process whose the entry point is a
      function provided by the user. It's an "image of the function".
]

The `spmd_block` class contains the following methods:

* [def Team information]
  `get_num_images`, `this_image`, `images_per_locality`
* [def Control statements]
  `sync_all`, `sync_images`

Here is a sample code summarizing the features offered by the `spmd_block` class.

    #include <hpx/lcos/spmd_block.hpp>

    void bulk_function(hpx::lcos::spmd_block block /* , arg0, arg1, ... */)
    {
        std::size_t num_images = block.get_num_images();
        std::size_t this_image = block.this_image();
        std::size_t images_per_locality = block.images_per_locality();

        /* Do some code */

        // Synchronize all images in the team
        block.sync_all();

        /* Do some code */

        // Synchronize image 0 and image 1
        block.sync_images(0,1);

        /* Do some code */

        std::vector<std::size_t> vec_images = {2,3,4};

        // Synchronize images 2, 3 and 4
        block.sync_images(vec_images);

        // Alternate call to synchronize images 2, 3 and 4
        block.sync_images(vec_images.begin(), vec_images.end());

        /* Do some code */

        // Non-blocking version of sync_all()
        hpx::future<void> event =
            block.sync_all(hpx::launch::async);

        // Callback waiting for 'event' to be ready before being scheduled
        hpx::future<void> cb =
            event.then(
              [](hpx::future<void>)
              {

                /* Do some code */

              });

        // Finally wait for the execution tree to be finished
        cb.get();
    }
    HPX_PLAIN_ACTION(bulk_test_function, bulk_test_action);

Then, in order to invoke the parallel section, call the
function 'define_spmd_block` specifying an arbitrary symbolic name and
indicating the number of images per locality to create.

    void bulk_function(hpx::lcos::spmd_block block, /* , arg0, arg1, ... */)
    {

    }
    HPX_PLAIN_ACTION(bulk_test_function, bulk_test_action);


    int main()
    {
        /* std::size_t arg0, arg1, ...; */

        bulk_action act;
        std::size_t images_per_locality = 4;

        // Instanciate the parallel section
        hpx::lcos::define_spmd_block(
            "some_name", images_per_locality, std::move(act) /*, arg0, arg1, ... */);

        return 0;
    }
[note In principle, the user should never call the `spmd_block` constructor.
      The `define_spmd_block` function is responsible of instanciating it and
      broadcasting it to each created image.
]
[endsect]

[section:spmd_views SPMD Multidimensionnal Views]
Object classes are defined as "container views" when the purpose is to
observe and/or to modify the values of a container using another perspective
than the one that caracterizes the container. For example, the values of an
`std::vector` object can be accessed via the expression `v[i]`. Container views
can be used, for example, when it is desired for those values to be "viewed" as
a 2D matrix that would have been flattened in a `std::vector`. The values would
be possibly accessible via the expression `vv(i,j)` which would call internally
the expression `v[k]`.

By default, the `partitioned_vector` class integrates 1-D views of its segments :

    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(double);

    hpx::partitioned_vector<double> v;

    // Create a 1-D view of the vector of segments
    auto vv = traits::segment(v.begin());

    // Access segment i
    std::vector<double> v = vv[i];

Our views are called "multi-dimensional" in the sense that they generalize
to N dimensions the purpose of `traits::segments()` in the 1-D case. Note that
in a parallel section, the 2-D expression `a(i,j) = b(i,j)` is quite confusing
because without convention, each of the images invoked will race to execute
the statement. For this reason, our views are not only multi-dimensional
but also "spmd-aware".

[note Spmd-awareness: The convention is simple. If an assignment statement
       contains a view subscript as an l-value, it is only and only the image
       holding the r-value who is evaluating the statement. (In MPI sense,
       it is called a Put operation]

[section:Subscripts Subscript-based operations]
Here are some examples of using subscripts in the 2-D view case:

    #include <hpx/components/containers/partitioned_vector/partitioned_vector_view.hpp>
    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(double);

    using Vec = hpx::partitioned_vector<double>;
    using View_2D = hpx::partitioned_vector_view<double,2>;

    /* Do some code */

    Vec v;

    // Parallel section (suppose 'block' an spmd_block instance)
    {
        std::size_t height, width;

        // Instanciate the view
        View_2D vv(block, v.begin(), v.end(), {height,width});

        // The l-value is a view subscript, the image that owns vv(1,0)
        // evaluates the assignment.
        vv(0,1) = vv(1,0);

        // The l-value is a view subscript, the image that owns the r-value
        // (result of expression 'std::vector<double>(4,1.0)') evaluates the
        // assignment : oops! race between all participating images.
        vv(2,3) = std::vector<double>(4,1.0);
    }
[endsect]

[section:ViewIterators Iterator-based operations]
Here are some examples of using iterators in the 3-D view case :

    #include <hpx/components/containers/partitioned_vector/partitioned_vector_view.hpp>
    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(int);

    using Vec = hpx::partitioned_vector<int>;
    using View_3D = hpx::partitioned_vector_view<int,3>;

    /* Do some code */

    Vec v1, v2;

    // Parallel section (suppose 'block' an spmd_block instance)
    {
        std::size_t sixe_x, size_y, size_z;

        // Instanciate the views
        View_3D vv1(block, v.begin(), v.end(), {sixe_x,size_y,size_z});
        View_3D vv2(block, v.begin(), v.end(), {sixe_x,size_y,size_z});

        // Save previous segments covered by vv1 into segments covered by vv2
        auto vv2_it = vv2.begin();
        auto vv1_it = vv1.cbegin();

        for(; vv2_it != vv2.end(); vv2_it++, vv1_it++)
        {
            // It's a Put operation
            *vv2_it = *vv1_it;
        }

        // Ensure that all images have performed their Put operations
        block.sync_all();

        // Ensure that only one image is putting updated data into the different
        // segments covered by vv1
        if(block.this_image() == 0)
        {
            int idx = 0;

            // Update all the segments covered by vv1
            for(auto i = vv1.begin(); i != vv1.end(); i++)
            {
                // It's a Put operation
                *i = std::vector<float>(elt_size,idx++);
            }
        }
    }

Here is an example that shows how to iterate only over segments
owned by the current image :

    #include <hpx/components/containers/partitioned_vector/partitioned_vector_view.hpp>
    #include <hpx/components/containers/partitioned_vector/partitioned_vector_local_view.hpp>
    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(float);

    using Vec = hpx::partitioned_vector<float>;
    using View_1D = hpx::partitioned_vector_view<float,1>;

    /* Do some code */

    Vec v;

    // Parallel section (suppose 'block' an spmd_block instance)
    {
        std::size_t num_segments;

        // Instanciate the view
        View_1D vv(block, v.begin(), v.end(), {num_segments});

        // Instanciate the local view from the view
        auto local_vv = hpx::local_view(vv);

        for ( auto i = localvv.begin(); i != localvv.end(); i++ )
        {
            std::vector<float> & segment = *i;

            /* Do some code */
        }

    }
[endsect]

[section:SubViews Instanciating Sub-views]
It is possible to construct views from other views : we call it sub-views. The
constraint nevertheless for the subviews is to retain the dimension and the
value type of the input view. Here is an example showing how to create a
sub-view.

    #include <hpx/components/containers/partitioned_vector/partitioned_vector_view.hpp>
    #include <hpx/include/partitioned_vector.hpp>

    // The following code generates all necessary boiler plate to enable the
    // remote creation of 'partitioned_vector' segments
    //
    ``[macroref HPX_REGISTER_PARTITIONED_VECTOR `HPX_REGISTER_PARTITIONED_VECTOR`]``(float);

    using Vec = hpx::partitioned_vector<float>;
    using View_2D = hpx::partitioned_vector_view<float,2>;

    /* Do some code */

    Vec v;

    // Parallel section (suppose 'block' an spmd_block instance)
    {
        std::size_t N = 20;
        std::size_t tilesize = 5;

        // Instanciate the view
        View_2D vv(block, v.begin(), v.end(), {N,N});

        // Instanciate the subview
        View_2D svv(
            block,&v(tilesize,0),&v(2*tilesize-1,tilesize-1),{tilesize,tilesize},{N,N});

        if(block.this_image() == 0)
        {
            // Equivalent to 'sv(tilesize,0) = 2.0f'
            svv(0,0) = 2.0f;

            // Equivalent to 'sv(2*tilesize-1,tilesize-1) = 3.0f'
            svv(tilesize-1,tilesize-1) = 3.0f;
        }

    }
[note The last parameter of the subview constructor is the size of the
      original view. If one would like to create a subview of the subview and so
      on, this parameter should stay unchanged. (`{N,N}` for the above example)
]
[endsect]

[endsect]

[endsect]
