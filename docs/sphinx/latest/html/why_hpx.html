

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Why HPX? &mdash; HPX 1.4.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quick start" href="quickstart.html" />
    <link rel="prev" title="Welcome to the HPX documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> HPX
          

          
            
            <img src="_static/HPX_STELLAR.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.4.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Why <em>HPX</em>?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#parallex-a-new-execution-model-for-future-architectures">ParalleX—a new execution model for future architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-hpx">What is <em>HPX</em>?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-makes-our-systems-slow">What makes our systems slow?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#technology-demands-new-response">Technology demands new response</a></li>
<li class="toctree-l2"><a class="reference internal" href="#governing-principles-applied-while-developing-hpx">Governing principles applied while developing <em>HPX</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="#focus-on-latency-hiding-instead-of-latency-avoidance">Focus on latency hiding instead of latency avoidance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#embrace-fine-grained-parallelism-instead-of-heavyweight-threads">Embrace fine-grained parallelism instead of heavyweight threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rediscover-constraint-based-synchronization-to-replace-global-barriers">Rediscover constraint-based synchronization to replace global barriers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adaptive-locality-control-instead-of-static-data-distribution">Adaptive locality control instead of static data distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prefer-moving-work-to-the-data-over-moving-data-to-the-work">Prefer moving work to the data over moving data to the work</a></li>
<li class="toctree-l2"><a class="reference internal" href="#favor-message-driven-computation-over-message-passing">Favor message driven computation over message passing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual.html">Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="additional_material.html">Additional material</a></li>
</ul>
<p class="caption"><span class="caption-text">Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="libs/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="libs/all_modules.html">All modules</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">API reference</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to <em>HPX</em></a></li>
</ul>
<p class="caption"><span class="caption-text">Other</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="about_hpx.html">About <em>HPX</em></a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">HPX</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Why <em>HPX</em>?</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/STEllAR-GROUP/hpx/blob/master/docs/sphinx/why_hpx.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="why-hpx">
<span id="id2"></span><h1>Why <em>HPX</em>?<a class="headerlink" href="#why-hpx" title="Permalink to this headline">¶</a></h1>
<p>Current advances in high performance computing (HPC) continue to suffer from the
issues plaguing parallel computation. These issues include, but are not limited
to, ease of programming, inability to handle dynamically changing workloads,
scalability, and efficient utilization of system resources. Emerging
technological trends such as multi-core processors further highlight limitations
of existing parallel computation models. To mitigate the aforementioned
problems, it is necessary to rethink the approach to parallelization models.
ParalleX contains mechanisms such as multi-threading, <a class="reference internal" href="terminology.html#term-parcel"><span class="xref std std-term">parcels</span></a>,
<a class="reference internal" href="terminology.html#term-agas"><span class="xref std std-term">global name space</span></a> support, percolation and <a class="reference internal" href="terminology.html#term-local-control-object"><span class="xref std std-term">local control
object</span></a>s (<a class="reference internal" href="terminology.html#term-lco"><span class="xref std std-term">LCO</span></a>). By design, ParalleX overcomes limitations of current
models of parallelism by alleviating contention, latency, overhead and
starvation. With ParalleX, it is further possible to increase performance by at
least an order of magnitude on challenging parallel algorithms, e.g., dynamic
directed graph algorithms and adaptive mesh refinement methods for astrophysics.
An additional benefit of ParalleX is fine-grained control of power usage,
enabling reductions in power consumption.</p>
<div class="section" id="parallex-a-new-execution-model-for-future-architectures">
<h2>ParalleX—a new execution model for future architectures<a class="headerlink" href="#parallex-a-new-execution-model-for-future-architectures" title="Permalink to this headline">¶</a></h2>
<p>ParalleX is a new parallel execution model that offers an alternative to
the conventional computation models, such as message passing. ParalleX
distinguishes itself by:</p>
<ul class="simple">
<li><p>Split-phase transaction model</p></li>
<li><p>Message-driven</p></li>
<li><p>Distributed shared memory (not cache coherent)</p></li>
<li><p>Multi-threaded</p></li>
<li><p>Futures synchronization</p></li>
<li><p><a class="reference internal" href="terminology.html#term-local-control-object"><span class="xref std std-term">Local Control Object</span></a>s (<a class="reference internal" href="terminology.html#term-lco"><span class="xref std std-term">LCO</span></a>s)</p></li>
<li><p>Synchronization for anonymous producer-consumer scenarios</p></li>
<li><p>Percolation (pre-staging of task data)</p></li>
</ul>
<p>The ParalleX model is intrinsically latency hiding, delivering an abundance of
variable-grained parallelism within a hierarchical namespace environment. The
goal of this innovative strategy is to enable future systems delivering very
high efficiency, increased scalability and ease of programming. ParalleX can
contribute to significant improvements in the design of all levels of computing
systems and their usage from application algorithms and their programming
languages to system architecture and hardware design together with their
supporting compilers and operating system software.</p>
</div>
<div class="section" id="what-is-hpx">
<h2>What is <em>HPX</em>?<a class="headerlink" href="#what-is-hpx" title="Permalink to this headline">¶</a></h2>
<p>High Performance ParalleX (<em>HPX</em>) is the first runtime system implementation of
the ParalleX execution model. The <em>HPX</em> runtime software package is a modular,
feature-complete, and performance-oriented representation of the ParalleX
execution model targeted at conventional parallel computing architectures, such
as SMP nodes and commodity clusters. It is academically developed and freely
available under an open source license. We provide <em>HPX</em> to the community for
experimentation and application to achieve high efficiency and scalability for
dynamic adaptive and irregular computational problems. <em>HPX</em> is a C++ library
that supports a set of critical mechanisms for dynamic adaptive resource
management and lightweight task scheduling within the context of a global
address space. It is solidly based on many years of experience in writing highly
parallel applications for HPC systems.</p>
<p>The two-decade success of the communicating sequential processes (CSP) execution
model and its message passing interface (MPI) programming model have been
seriously eroded by challenges of power, processor core complexity, multi-core
sockets, and heterogeneous structures of GPUs. Both efficiency and scalability
for some current (strong scaled) applications and future Exascale applications
demand new techniques to expose new sources of algorithm parallelism and exploit
unused resources through adaptive use of runtime information.</p>
<p>The ParalleX execution model replaces CSP to provide a new computing paradigm
embodying the governing principles for organizing and conducting highly
efficient scalable computations greatly exceeding the capabilities of today’s
problems. <em>HPX</em> is the first practical, reliable, and performance-oriented
runtime system incorporating the principal concepts of the ParalleX model
publicly provided in open source release form.</p>
<p><em>HPX</em> is designed by the <a class="reference external" href="https://stellar-group.org">STE||AR</a> Group (<strong>S</strong>ystems <strong>T</strong>echnology,
<strong>E</strong>mergent Para<strong>ll</strong>elism, and <strong>A</strong>lgorithm <strong>R</strong>esearch) at
<a class="reference external" href="https://www.lsu.edu">Louisiana State University (LSU)</a>’s <a class="reference external" href="https://www.cct.lsu.edu">Center for Computation and Technology (CCT)</a> to enable developers to exploit the full processing power of
many-core systems with an unprecedented degree of parallelism. <a class="reference external" href="https://stellar-group.org">STE||AR</a> is a
research group focusing on system software solutions and scientific application
development for hybrid and many-core hardware architectures.</p>
<p>For more information about the <a class="reference external" href="https://stellar-group.org">STE||AR</a> Group, see <a class="reference internal" href="about_hpx/people.html#people"><span class="std std-ref">People</span></a>.</p>
</div>
<div class="section" id="what-makes-our-systems-slow">
<h2>What makes our systems slow?<a class="headerlink" href="#what-makes-our-systems-slow" title="Permalink to this headline">¶</a></h2>
<p>Estimates say that we currently run our computers at well below 100% efficiency.
The theoretical peak performance (usually measured in
<a class="reference external" href="http://en.wikipedia.org/wiki/FLOPS">FLOPS</a>—floating point operations per
second) is much higher than any practical peak performance reached by any
application. This is particularly true for highly parallel hardware. The more
hardware parallelism we provide to an application, the better the application
must scale in order to efficiently use all the resources of the machine. Roughly
speaking, we distinguish two forms of scalability: strong scaling (see
<a class="reference external" href="http://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s Law</a>) and weak scaling
(see <a class="reference external" href="http://en.wikipedia.org/wiki/Gustafson%27s_law">Gustafson’s Law</a>). Strong
scaling is defined as how the solution time varies with the number of processors
for a fixed <strong>total</strong> problem size. It gives an estimate of how much faster we
can solve a particular problem by throwing more resources at it. Weak scaling is
defined as how the solution time varies with the number of processors for a
fixed problem size <strong>per processor</strong>. In other words, it defines how much more
data can we process by using more hardware resources.</p>
<p>In order to utilize as much hardware parallelism as possible an application must
exhibit excellent strong and weak scaling characteristics, which requires a high
percentage of work executed in parallel, i.e., using multiple threads of
execution. Optimally, if you execute an application on a hardware resource with
N processors it either runs N times faster or it can handle N times more data.
Both cases imply 100% of the work is executed on all available processors in
parallel. However, this is just a theoretical limit. Unfortunately, there are
more things that limit scalability, mostly inherent to the hardware
architectures and the programming models we use. We break these limitations into
four fundamental factors that make our systems <em>SLOW</em>:</p>
<ul class="simple">
<li><p><strong>S</strong>tarvation occurs when there is insufficient concurrent work available to
maintain high utilization of all resources.</p></li>
<li><p><strong>L</strong>atencies are imposed by the time-distance delay intrinsic to accessing
remote resources and services.</p></li>
<li><p><strong>O</strong>verhead is work required for the management of parallel actions and
resources on the critical execution path, which is not necessary in a
sequential variant.</p></li>
<li><p><strong>W</strong>aiting for contention resolution is the delay due to the lack of
availability of oversubscribed shared resources.</p></li>
</ul>
<p>Each of those four factors manifests itself in multiple and different ways; each
of the hardware architectures and programming models expose specific forms.
However, the interesting part is that all of them are limiting the scalability of
applications no matter what part of the hardware jungle we look at. Hand-helds,
PCs, supercomputers, or the cloud, all suffer from the reign of the 4 horsemen:
<strong>S</strong>tarvation, <strong>L</strong>atency, <strong>O</strong>verhead, and <strong>C</strong>ontention. This
realization is very important as it allows us to derive the criteria for
solutions to the scalability problem from first principles, and it allows us to
focus our analysis on very concrete patterns and measurable metrics. Moreover,
any derived results will be applicable to a wide variety of targets.</p>
</div>
<div class="section" id="technology-demands-new-response">
<h2>Technology demands new response<a class="headerlink" href="#technology-demands-new-response" title="Permalink to this headline">¶</a></h2>
<p>Today’s computer systems are designed based on the initial ideas of
<a class="reference external" href="http://qss.stanford.edu/~godfrey/vonNeumann/vnedvac.pdf">John von Neumann</a>, as
published back in 1945, and later extended by the
<a class="reference external" href="http://en.wikipedia.org/wiki/Harvard_architecture">Harvard architecture</a>. These
ideas form the foundation, the execution model, of computer systems we use
currently. However, a new response is required in the light of the demands
created by today’s technology.</p>
<p>So, what are the overarching objectives for designing systems allowing for
applications to scale as they should? In our opinion, the main objectives are:</p>
<ul class="simple">
<li><p>Performance: as previously mentioned, scalability and efficiency are the main criteria
people are interested in.</p></li>
<li><p>Fault tolerance: the low expected mean time between failures (<a class="reference external" href="http://en.wikipedia.org/wiki/Mean_time_between_failures">MTBF</a>) of future systems
requires embracing faults, not trying to avoid them.</p></li>
<li><p>Power: minimizing energy consumption is a must as it is one of the major cost
factors today, and will continue to rise in the future.</p></li>
<li><p>Generality: any system should be usable for a broad set of use cases.</p></li>
<li><p>Programmability: for programmer this is a very important objective,
ensuring long term platform stability and portability.</p></li>
</ul>
<p>What needs to be done to meet those objectives, to make applications scale
better on tomorrow’s architectures? Well, the answer is almost obvious: we need
to devise a new execution model—a set of governing principles for the holistic
design of future systems—targeted at minimizing the effect of the outlined
<strong>SLOW</strong> factors. Everything we create for future systems, every design decision
we make, every criteria we apply, have to be validated against this single,
uniform metric. This includes changes in the hardware architecture we
prevalently use today, and it certainly involves new ways of writing software,
starting from the operating system, runtime system, compilers, and at the
application level. However, the key point is that all those layers have to be
co-designed; they are interdependent and cannot be seen as separate facets. The
systems we have today have been evolving for over 50 years now. All layers
function in a certain way, relying on the other layers to do so. But we do not
have the time to wait another 50 years for a new coherent system to evolve.
The new paradigms are needed now—therefore, co-design is the key.</p>
</div>
<div class="section" id="governing-principles-applied-while-developing-hpx">
<h2>Governing principles applied while developing <em>HPX</em><a class="headerlink" href="#governing-principles-applied-while-developing-hpx" title="Permalink to this headline">¶</a></h2>
<p>As it turn out, we do not have to start from scratch. Not everything has to be
invented and designed anew. Many of the ideas needed to combat the 4 horsemen
already exist, many for more than 30 years. All it takes is to gather
them into a coherent approach. We’ll highlight some of the derived principles we
think to be crucial for defeating <strong>SLOW</strong>. Some of those are focused on
high-performance computing, others are more general.</p>
</div>
<div class="section" id="focus-on-latency-hiding-instead-of-latency-avoidance">
<h2>Focus on latency hiding instead of latency avoidance<a class="headerlink" href="#focus-on-latency-hiding-instead-of-latency-avoidance" title="Permalink to this headline">¶</a></h2>
<p>It is impossible to design a system exposing zero latencies. In an effort to
come as close as possible to this goal many optimizations are mainly targeted
towards minimizing latencies. Examples for this can be seen everywhere, such as
low latency network technologies like <a class="reference external" href="http://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a>, caching memory hierarchies in all
modern processors, the constant optimization of existing <a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a> implementations
to reduce related latencies, or the data transfer latencies intrinsic to the way
we use <a class="reference external" href="http://en.wikipedia.org/wiki/GPGPU">GPGPUs</a> today. It is important to
note that existing latencies are often tightly related to some resource having
to wait for the operation to be completed. At the same time it would be
perfectly fine to do some other, unrelated work in the meantime, allowing the
system to hide the latencies by filling the idle-time with useful work. Modern
systems already employ similar techniques (pipelined instruction execution in
the processor cores, asynchronous input/output operations, and many more). What
we propose is to go beyond anything we know today and to make latency hiding an
intrinsic concept of the operation of the whole system stack.</p>
</div>
<div class="section" id="embrace-fine-grained-parallelism-instead-of-heavyweight-threads">
<h2>Embrace fine-grained parallelism instead of heavyweight threads<a class="headerlink" href="#embrace-fine-grained-parallelism-instead-of-heavyweight-threads" title="Permalink to this headline">¶</a></h2>
<p>If we plan to hide latencies even for very short operations, such as fetching
the contents of a memory cell from main memory (if it is not already cached), we
need to have very lightweight threads with extremely short context switching
times, optimally executable within one cycle. Granted, for mainstream
architectures, this is not possible today (even if we already have special
machines supporting this mode of operation, such as the <a class="reference external" href="http://en.wikipedia.org/wiki/Cray_XMT">Cray XMT</a>). For conventional systems, however,
the smaller the overhead of a context switch and the finer the granularity of
the threading system, the better will be the overall system utilization and its
efficiency. For today’s architectures we already see a flurry of libraries
providing exactly this type of functionality: non-pre-emptive, task-queue based
parallelization solutions, such as <a class="reference external" href="https://www.threadingbuildingblocks.org/">Intel Threading Building Blocks (TBB)</a>, <a class="reference external" href="https://msdn.microsoft.com/en-us/library/dd492418.aspx">Microsoft Parallel Patterns Library (PPL)</a>, <a class="reference external" href="https://software.intel.com/en-us/articles/intel-cilk-plus/">Cilk++</a>, and many others.
The possibility to suspend a current task if some preconditions for its
execution are not met (such as waiting for I/O or the result of a different
task), seamlessly switching to any other task which can continue, and to
reschedule the initial task after the required result has been calculated, which
makes the implementation of latency hiding almost trivial.</p>
</div>
<div class="section" id="rediscover-constraint-based-synchronization-to-replace-global-barriers">
<h2>Rediscover constraint-based synchronization to replace global barriers<a class="headerlink" href="#rediscover-constraint-based-synchronization-to-replace-global-barriers" title="Permalink to this headline">¶</a></h2>
<p>The code we write today is riddled with implicit (and explicit) global barriers.
By “global barriers,” we mean the synchronization of the control flow between
several (very often all) threads (when using <a class="reference external" href="https://openmp.org/wp/">OpenMP</a>) or processes (<a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>).
For instance, an implicit global barrier is inserted after each loop
parallelized using <a class="reference external" href="https://openmp.org/wp/">OpenMP</a> as the system synchronizes the threads used to
execute the different iterations in parallel. In <a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a> each of the
communication steps imposes an explicit barrier onto the execution flow as
(often all) nodes have to be synchronized. Each of those barriers is like the eye
of a needle the overall execution is forced to be squeezed through. Even
minimal fluctuations in the execution times of the parallel threads (jobs)
causes them to wait. Additionally, it is often only one of the executing threads
that performs the actual reduce operation, which further impedes parallelism. A closer
analysis of a couple of key algorithms used in science applications reveals that
these global barriers are not always necessary. In many cases it is sufficient
to synchronize a small subset of the threads. Any operation should proceed
whenever the preconditions for its execution are met, and only those. Usually
there is no need to wait for iterations of a loop to finish before you can
continue calculating other things; all you need is to complete the iterations
that produce the required results for the next operation. Good
bye global barriers, hello constraint based synchronization! People have been
trying to build this type of computing (and even computers) since the 1970s.
The theory behind what they did is based on ideas around static and
dynamic dataflow. There are certain attempts today to get back to those ideas
and to incorporate them with modern architectures. For instance, a lot of work
is being done in the area of constructing dataflow-oriented execution trees. Our
results show that employing dataflow techniques in combination with the other
ideas, as outlined herein, considerably improves scalability for many problems.</p>
</div>
<div class="section" id="adaptive-locality-control-instead-of-static-data-distribution">
<h2>Adaptive locality control instead of static data distribution<a class="headerlink" href="#adaptive-locality-control-instead-of-static-data-distribution" title="Permalink to this headline">¶</a></h2>
<p>While this principle seems to be a given for single desktop or laptop computers
(the operating system is your friend), it is everything but ubiquitous on modern
supercomputers, which are usually built from a large number of separate nodes
(i.e., Beowulf clusters), tightly interconnected by a high-bandwidth, low-latency
network. Today’s prevalent programming model for those is MPI, which does not
directly help with proper data distribution, leaving it to the programmer to
decompose the data to all of the nodes the application is running on. There are
a couple of specialized languages and programming environments based on <a class="reference external" href="https://www.pgas.org/">PGAS</a>
(Partitioned Global Address Space) designed to overcome this limitation, such as
<a class="reference external" href="https://chapel.cray.com/">Chapel</a>, <a class="reference external" href="https://x10-lang.org/">X10</a>, <a class="reference external" href="https://upc.lbl.gov/">UPC</a>, or <a class="reference external" href="https://labs.oracle.com/projects/plrg/Publications/index.html">Fortress</a>. However, all systems based on PGAS
rely on static data distribution. This works fine as long as this static data
distribution does not result in heterogeneous workload distributions or other
resource utilization imbalances. In a distributed system these imbalances can be
mitigated by migrating part of the application data to different localities
(nodes). The only framework supporting (limited) migration today is <a class="reference external" href="https://charm.cs.uiuc.edu/">Charm++</a>.
The first attempts towards solving related problem go back decades as well, a
good example is the <a class="reference external" href="http://en.wikipedia.org/wiki/Linda_(coordination_language)">Linda coordination language</a>. Nevertheless,
none of the other mentioned systems support data migration today, which forces
the users to either rely on static data distribution and live with the related
performance hits or to implement everything themselves, which is very tedious
and difficult. We believe that the only viable way to flexibly support dynamic
and adaptive <a class="reference internal" href="terminology.html#term-locality"><span class="xref std std-term">locality</span></a> control is to provide a global, uniform address
space to the applications, even on distributed systems.</p>
</div>
<div class="section" id="prefer-moving-work-to-the-data-over-moving-data-to-the-work">
<h2>Prefer moving work to the data over moving data to the work<a class="headerlink" href="#prefer-moving-work-to-the-data-over-moving-data-to-the-work" title="Permalink to this headline">¶</a></h2>
<p>For the best performance it seems obvious to minimize the amount of bytes
transferred from one part of the system to another. This is true on all levels.
At the lowest level we try to take advantage of processor memory caches, thus,
minimizing memory latencies. Similarly, we try to amortize the data transfer
time to and from <a class="reference external" href="http://en.wikipedia.org/wiki/GPGPU">GPGPUs</a> as much as
possible. At high levels we try to minimize data transfer between different
nodes of a cluster or between different virtual machines on the cloud. Our
experience (well, it’s almost common wisdom) shows that the amount of bytes
necessary to encode a certain operation is very often much smaller than the
amount of bytes encoding the data the operation is performed upon. Nevertheless,
we still often transfer the data to a particular place where we execute the
operation just to bring the data back to where it came from afterwards. As an
example let’s look at the way we usually write our applications for clusters
using MPI. This programming model is all about data transfer between nodes.
MPI is the prevalent programming model for clusters, and it is fairly
straightforward to understand and to use. Therefore, we often write
applications in a way that accommodates this model, centered around data transfer.
These applications usually work well for smaller problem sizes and for regular
data structures. The larger the amount of data we have to churn and the more
irregular the problem domain becomes, the worse the overall machine
utilization and the (strong) scaling characteristics become. While it is not impossible
to implement more dynamic, data driven, and asynchronous applications using
MPI, it is somewhat difficult to do so. At the same time, if we look at
applications that prefer to execute the code close to the <a class="reference internal" href="terminology.html#term-locality"><span class="xref std std-term">locality</span></a> where the
data was placed, i.e., utilizing active messages (for instance based on
<a class="reference external" href="https://charm.cs.uiuc.edu/">Charm++</a>), we see better asynchrony, simpler application codes, and improved
scaling.</p>
</div>
<div class="section" id="favor-message-driven-computation-over-message-passing">
<h2>Favor message driven computation over message passing<a class="headerlink" href="#favor-message-driven-computation-over-message-passing" title="Permalink to this headline">¶</a></h2>
<p>Today’s prevalently used programming model on parallel (multi-node) systems is
MPI. It is based on message passing, as the name implies, which means that
the receiver has to be aware of a message about to come in. Both codes, the
sender and the receiver, have to synchronize in order to perform the
communication step. Even the newer, asynchronous interfaces require explicitly
coding the algorithms around the required communication scheme. As a result, everything
but the most trivial MPI applications spends a considerable amount of time
waiting for incoming messages, thus, causing starvation and latencies to impede
full resource utilization. The more complex and more dynamic the data structures
and algorithms become, the larger the adverse effects. The community discovered
message-driven and data-driven methods of implementing algorithms a long
time ago, and systems such as <a class="reference external" href="https://charm.cs.uiuc.edu/">Charm++</a> have already integrated active
messages demonstrating the validity of the concept. Message-driven computation
allows for sending messages without requiring the receiver to actively wait for
them. Any incoming message is handled asynchronously and triggers the encoded
action by passing along arguments and—possibly—continuations. <em>HPX</em> combines
this scheme with work-queue based scheduling as described above, which allows
the system to almost completely overlap any communication with useful work,
thereby minimizing latencies.</p>
</div>
</div>


           </div>
           
          </div>
          <!-- Copyright (c) 2018 Mikael Simberg

     Distributed under the Boost Software License, Version 1.0. (See accompanying
     file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt) -->
<!-- Make HPX inspect tool happy: hpxinspect:nounlinked -->

<footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="quickstart.html" class="btn btn-neutral float-right" title="Quick start" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to the HPX documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008-2019, The STE||AR Group
      
        <span class="commit">
          Revision <code>1e2a841</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
 

SPDX-License-Identifier: BSL-1.0
Distributed under the Boost Software License, Version 1.0. (See accompanying file
LICENSE_1_0.txt or copy at
<a href="http://www.boost.org/LICENSE_1_0.txt">http://www.boost.org/LICENSE_1_0.txt</a>)



</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>