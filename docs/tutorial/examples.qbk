[/=============================================================================
    Copyright (C) 2012 Adrian Serio
    Copyright (C) 2012 Vinay C Amatya

    Distributed under the Boost Software License, Version 1.0. (See accompanying
    file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:examples Examples]

The following sections of our tutorial analyzes some examples to help you get
familiar with the __hpx__ style of programming. We start off with simple examples
that utilize basic __hpx__ elements and then begin to expose the reader to the more
complex, yet powerful, __hpx__ concepts.

[note The instructions for building and running the examples currently
      only cover Unix variants.]

[/Proofread by:]
[/Adrian Serio 3-13-12]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]
[section:fibonacci Fibonacci]

The Fibonacci sequence is a sequence of numbers starting with 0 and 1 where
every subsequent number is the sum of the previous two numbers. In this
example, we will use __hpx__ to calculate the value of the n-th element of the
Fibonacci sequence. In order to compute this problem in parallel, we will use
a facility known as a Future.

As shown in the [link examples.future_schematics figure] below, a Future
encapsulates a delayed computation. It acts as
a proxy for a result initially not known, most of the time because the
computation of the result has not completed yet. The Future synchronizes the
access of this value by optionally suspending any __hpx__-threads requesting the
result until the value is available. When a Future is created, it spawns a
new __hpx__-thread (either remotely with a parcel or locally by placing it into
the thread queue) which, when run, will execute the action associated with
the Future. The arguments of the action are bound when the Future is created.

[fig future_schematics.png..Schematic of a Future execution..examples.future_schematics]

Once the action has finished executing, a write operation is performed on the
Future. The write operation marks the Future as completed, and optionally
stores data returned by the action. When the result of the delayed computation
is needed, a read operation is performed on the Future. If the Future's action
hasn't completed when a read operation is performed on it, the reader
__hpx__-thread is suspended until the Future is ready. The Future facility allows
__hpx__ to schedule work early in a program so that when the function value is
needed it will already be calculated and available. We use this property in our
Fibonacci example below to enable its parallel execution.

[heading Setup]

[teletype]

The source code for this example can be found here:
[hpx_link examples/quickstart/fibonacci.cpp..fibonacci.cpp].

To compile this program, go to your __hpx__ build directory (see __getting_started__
for information on configuring and building __hpx__) and enter:

``
    $ make examples.quickstart.fibonacci
``

To run the program type:

``
    $ ./bin/fibonacci
``

This should print (time should be approximate):

``
    fibonacci(10) == 55
    elapsed time: 0.00186288 [s]
``

This run used the default settings, which calculate the tenth element of the
Fibonacci sequence. To declare which Fibonacci value you want to calculate, use
the [^--n-value] option. Additionally you can use the
[hpx_cmdline [^--hpx:threads]] option to declare how many OS-threads you
wish to use when running the program. For instance, running:

```
    $ ./bin/fibonacci --n-value 20 ``[hpx_cmdline --hpx:threads]`` 4
```

Will yield:

``
    fibonacci(20) == 6765
    elapsed time: 0.233827 [s]
``

[c++]

[heading Walkthrough]

[c++]

Now that you have compiled and run the code, let's look at how the code works.
Since this code is written in C++, we will begin with the `main()` function.
Here you can see that in __hpx__, `main()` is only used to initialize the runtime
system. It is important to note that application-specific command line options
are defined here. __hpx__ uses __boost_program_options__ for command line
processing. You can see that our programs [^--n-value] option is set by calling
the `add_options()` method on an instance of
`boost::program_options::options_description`.  The default value of the
variable is set to 10. This is why when we ran the program for the first time
without using the [^--n-value] option the program returned the 10th value of
the Fibonacci sequence. The constructor argument of the description is the text
that appears when a user uses the [hpx_cmdline [^--help]] option to see what
command line options are available. `HPX_APPLICATION_STRING` is a macro that
expands to a string constant containing the name of the __hpx__ application
currently being compiled.

In __hpx__ `main()` is used to initialize the runtime system and pass the command
line arguments to the program. If you wish to add command line options to
your program you would add them here using the instance of the Boost
class `options_description`, and invoking the public member function
`.add_options()` (see __boost_doc__ or the __fibonacci_example__
for more details). `hpx::init()` calls `hpx_main()` after setting up
__hpx__, which is where the logic of our program is encoded.

[import ../../examples/quickstart/fibonacci.cpp]
[fib_main]

The [funcref hpx::init]`()` function in `main()` starts the runtime system, and invokes
[funcref hpx_main]`()` as the first __hpx__-thread. Below we can see that the basic program
is simple. The command line option [^--n-value] is read in, a timer
([classref hpx::util::high_resolution_timer]) is set up to record the time it takes to
do the computation, the fibonacci action is invoked synchronously, and the answer is
printed out.

[fib_hpx_main]

Upon a closer look we see that the [classref hpx::lcos::future]`<>` we created is assigned
the return of [funcref hpx::lcos::async]`<fibonacci_action>(`[funcref hpx::find_here]`(), n)`.
[funcref hpx::lcos::async]`<>()` takes an action, in this case `fibonacci_action`, and
asynchronously kicks of the computation of the action, returning a future which
represents the result of the computation. But wait, what is an action? And what
is this `fibonacci_action`? For starters, an action is a wrapper for a
function. By wrapping functions, __hpx__ can send packets of work to different
processing units. These vehicles allow users to calculate work now, later, or
on certain nodes. The first argument to [funcref hpx::lcos::async]`<>()` is the location
where the action should be run. In this case, we just want to run the action on
the machine that we are currently on, so we use [funcref hpx::find_here]`()`. To further
understand this we turn to the code to find where `fibonacci_action` was
defined:

[fib_action]

A plain action is the most basic form of action. Plain actions wrap simple global
functions which are not associated with any particular object (we will discuss
other types of actions in the __accumulator_example__).
In this block of code the function `fibonacci()` is declared. After the
declaration, the function is wrapped in an action in the declaration
[macroref HPX_PLAIN_ACTION `HPX_PLAIN_ACTION`]. This function takes two aruments: the name of the
function that is to be wrapped and the name of the action that you are
creating.

This picture should now start making sense. The function `fibonacci()` is
wrapped in an action `fibonacci_action`, which was spawned
by [funcref hpx::lcos::async]`<>()`, which returns a future. Now, lets look at the
function `fibonacci()`:

[fib_func]

This block of code is much more straightforward. First, `if (n < 2)`,
meaning n is 0 or 1, then we return 0 or 1 (recall the first element
of the Fibonacci sequence is 0 and the second is 1). If n is larger than 1, then
we spawn two futures, `n1` and `n2`. Each of these futures represents an asynchronous,
recursive call to `fibonacci()`. After we've created both futures, we wait for both
of them to finish computing, and then we add them together, and return that value
as our result. The recursive call tree will continue until n is equal to 0 or 1,
at which point the value can be returned because it is implicitly known.
When this termination condition is reached, the futures can then be added up,
producing the n-th value of the Fibonacci sequence.

[endsect] [/Fibonacci]

[/Last Edit: 4-13-12]

[/Proofread by:]
[/Adrian Serio 3-13-12]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]
[section:hello_world Hello World]

[teletype]

This program will print out a hello world message on every OS-thread on every
locality. The output will look something like this:

``
    hello world from OS-thread 1 on locality 0
    hello world from OS-thread 1 on locality 1
    hello world from OS-thread 0 on locality 0
    hello world from OS-thread 0 on locality 1
``

[heading Setup]

The source code for this example can be found here:
[hpx_link examples/quickstart/hello_world.cpp..hello_world.cpp].

To compile this program, go to your __hpx__ build directory (see __getting_started__
for information on configuring and building __hpx__) and enter:

``
    $ make examples.quickstart.hello_world
``

To run the program type:

``
    $ ./bin/hello_world
``

This should print:

``
    hello world from OS-thread 0 on locality 0
``

To use more OS-threads use the command line option [hpx_cmdline [^--hpx:threads]]
and type the number of threads that you wish to use. For example, typing:

```
    $ ./bin/hello_world ``[hpx_cmdline --hpx:threads]`` 2
```

will yield:

``
    hello world from OS-thread 1 on locality 0
    hello world from OS-thread 0 on locality 0
``

Notice how the ordering of the two print statements will change with
subsequent runs. To run this program on multiple localities please see the
__pbs__ documentation.

[heading Walkthrough]

[c++]

Now that you have compiled and run the code, lets look at how the code works,
beginning with `main()`:

[import ../../examples/quickstart/hello_world.cpp]
[hello_world_hpx_main]

In this excerpt of the code we again see the use of futures. This time
the futures are stored in a vector so that they can easily be accessed.
[funcref hpx::lcos::wait]`()` is a family of functions that wait on for an `std::vector<>`
of futures to become ready. In this piece of code, we are using the synchronous
version of [funcref hpx::lcos::wait]`()`, which takes one argument (the `std::vector<>` of
futures to wait on). This function will not return until all the futures in the
vector have been executed.

In the __fibonacci_example__, we used [funcref hpx::find_here]`()` to specified the target'
of our actions. Here, we instead use [funcref hpx::find_all_localities]`()`, which returns
an `std::vector<>` containing the identifiers of all the machines in the system,
including the one that we are on.

As in the __fibonacci_example__ our futures are set using [funcref hpx::lcos::async]`<>()`.
The `hello_world_foreman_action` is declared here:

[hello_world_action_wrapper]

Another way of thinking about this wrapping technique is as follows: functions
(the work to be done) are wrapped in actions, and actions can be executed
locally or remotely (e.g. on another machine participating in the computation).

Now it is time to look at the `hello_world_foreman()` function which was wrapped in
the action above:

[hello_world_foreman]

Now, before we discuss `hello_world_foreman()`, let's talk about the
[funcref hpx::lcos::wait]`()` function. [funcref hpx::lcos::wait]`()` provides a way to make sure
that all of the futures have finished being calculated without having to call
[memberref hpx::lcos::future::get]`()` for each one. The version of [funcref hpx::lcos::wait]`()` used here performs an
non-blocking wait, which acts on an `std::vector<>`. It queries the state of
the futures, waiting for them to finish. Whenever a future becomes marked as
ready, [funcref hpx::lcos::wait]`()` invokes a callback function provided by the user,
supplying the callback function with the index of the future in the
`std::vector<>` and the result of the future.

In `hello_world_foreman()`, an `std::set<>` called `attendance` keeps track of
which OS-threads have printed out the hello world message. When the OS-thread
prints out the statement, the future is marked as ready, and
[funcref hpx::lcos::wait]`()` invokes the callback function, in this case a C+11 lambda.
This lambda erases the OS-threads id from the set `attendance`, thus letting
`hello_world_foreman()` know which OS-threads still need to print out hello
world. However, if the future returns a value of -1, the future executed on an
OS-thread which has already printed out hello world. In this case, we have to
try again by rescheduling the future in the next round. We do this by leaving
the OS-thread id in `attendance`.

Finally, let us look at `hello_world_worker()`. Here, `hello_world_worker()`
checks to see if it is on the target OS-thread. If it is executing on the
correct OS-thread, it prints out the hello world message and returns the
OS-thread id to [funcref hpx::lcos::wait]`()` in `hello_world_foreman()`. If it is
not executing on the correct OS-thread, it returns a value of -1, which causes
`hello_world_foreman()` to leave the OS-thread id in `attendance`.

[hello_world_worker]

Because __hpx__ features work stealing task schedulers, there is no way to guarantee
that an action will be scheduled on a particular OS-thread. This is why we must
use a guess-and-check approach.

[endsect] [/Hello World]

[/Proofread by:]
[/Adrian Serio 3-13-12]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]
[section:accumulator Accumulator]

The accumulator example demonstrates the use of components. Components are C++
classes that expose methods as a type of __hpx__ action. These actions are called
component actions.

Components are globally named, meaning that a component action can be called
remotely (e.g.  from another machine). There are two accumulator examples in
__hpx__; managed_accumulator and simple_accumulator (we will talk more about the
differences between the two later). This tutorial will examine the
managed_accumulator variant.

In the __fibonacci_example__ and the
__hello_world_example__, we introduced plain actions, which wrapped global
functions.  The target of a plain action is an identifier which refers to a
particular machine involved in the computation. For plain actions, the target
is the machine where the action will be executed.

Component actions, however, do not target machines. Instead, they target
component instances. The instance may live on the machine that we've invoked
the component action from, or it may live on another machine.

The component in this example exposes three different functions:

* `reset()` - Resets the accumulator value to 0.
* `add(arg)` - Adds `arg` to the accumulators value.
* `query()` - Queries the value of the accumulator.

This example creates an instance of the accumulator, and then allows the user
to enter commands at a prompt, which subsequently invoke actions on the
accumulator instance.

[heading Setup]

[teletype]

The source code for this example can be found here:
[hpx_link examples/accumulator/accumulators..accumulators].

To compile this program, go to your __hpx__ build directory (see __getting_started__
for information on configuring and building __hpx__) and enter:

``
    $ make examples.accumulator.managed_accumulator
``

To run the program type:


``
    ./bin/managed_accumulator_client
``

Once the program starts running, it will print the following prompt and then
wait for input. An example session is given below:

``
    commands: reset, add [amount], query, help, quit
    > add 5
    > add 10
    > query
    15
    > add 2
    > query
    17
    > reset
    > add 1
    > query
    1
    > quit
``

[heading Walkthrough]

[c++]

Now, let's take a look at the source code of the managed_accumulator example. This
example consists of two parts: an __hpx__ component library (a library that exposes
an __hpx__ component) and a client application which uses the library. This
walkthrough will cover the __hpx__ component library. The code for the client
application can be found here:
[hpx_link examples/accumulator/managed_accumulator_client.cpp..managed_accumulator_client.cpp].

An __hpx__ component is represented by three C++ classes:

* [*A server class] - The implementation of the components functionality.
* [*A stubs class] - A lower-level interface to instances of the component.
* [*A client class] - A high-level interface that acts as a proxy for an
  instance of the component.

Typically, these three classes all have the same name, but stubs and server
classes usually live in different sub-namespaces (`server` and `stubs`
respectively).  For example, the full names of the three classes in
managed_accumulator are:

* `examples::server::managed_accumulator` (server class)
* `examples::stubs::managed_accumulator` (stubs class)
* `examples::managed_accumulator` (client class)

[heading The Server Class]

The following code is from:
[hpx_link examples/accumulator/accumulators/server/managed_accumulator.hpp..server/managed_accumulator.hpp].

All __hpx__ component server classes must inherit publicly from an __hpx__ component
base class. There are currently two component base classes:

* [classref hpx::components::managed_component_base]`<>` - Managed components are components
  which are allocated in bulk by __hpx__. Managed components are more efficient if
  you are creating a large number (e.g. hundreds or more per machine) of
  component instances.
* [classref hpx::components::simple_component_base]`<>` - Simple components are components
  which are allocated individually by __hpx__. Simple components are more efficient
  if you are creating a small number (e.g. only a handful per machine) of
  component instances.

The managed_accumulator component is a managed component, because it inherits
from [classref hpx::components::managed_component_base]`<>`, as we can see in the following
snippet:

[import ../../examples/accumulator/accumulators/server/managed_accumulator.hpp]
[managed_accumulator_server_inherit]

Our accumulator class will need a data member to store its value in, so let's
declare a data member:

[managed_accumulator_server_data_member]

The `boost::atomic<>` template class is from the __boost_atomic__ library. We use
this class to ensure thread-safety in our example. Think of this data member as
a thread-safe, 64-bit unsigned integer.

The constructor for this class simply initializes `value_` to 0:

[managed_accumulator_server_ctor]

Next, let's look at the three methods of this component that we will be exposing
as component actions:

[managed_accumulator_methods]

Before we can define the action types for these methods, we first need to
associate an action code (an arbitrary, unique integral value) with each method
that we are going to expose as an action:

[managed_accumulator_action_codes]

Here are the action types. These types wrap the methods we're exposing. The
wrapping technique is very similar to the one used in the __fibonacci_example__
and the __hello_world_example__:

[managed_accumulator_action_types]

The last piece of code in the server class header is the declaration of the
action type registration code:

[managed_accumulator_registration_declarations]

[note The code above must be placed in the global namespace.]

The rest of the registration code is in
[hpx_link examples/accumulator/accumulators/managed_accumulator.cpp..managed_accumulator.cpp].

[import ../../examples/accumulator/accumulators/managed_accumulator.cpp]
[managed_accumulator_registration_definitions]

[note The code above must be placed in the global namespace.]

[heading The Stubs Class]

The following code is from
[hpx_link examples/accumulator/accumulators/stubs/managed_accumulator.hpp..stubs/managed_accumulator.hpp].

All stubs classes must inherit from the stubs base class, [classref hpx::components::stub_base]`<>`:

[import ../../examples/accumulator/accumulators/stubs/managed_accumulator.hpp]
[managed_accumulator_stubs_inherit]

The stubs class contains helper functions which invoke actions on component
instances. There are a few different ways of invoking actions:

* [*Non-blocking]: For actions which don't have return types, or when we do not
  care about the result of an action, we can invoke the action using
  fire-and-forget semantics. This means that once we have asked __hpx__ to compute
  the action, we forget about it completely and continue with our computation.
  We use [funcref hpx::applier::apply]`<>()` instead of
  [funcref hpx::lcos::async]`<>()` to invoke an action in a non-blocking fashion.
  Here's an example from the managed_accumulator stubs class:

[managed_accumulator_stubs_reset_non_blocking]

* [*Asynchronous]: Futures, as demonstrated in __fibonacci_example__ and the
  __hello_world_example__, enable asynchronous action invocation. Here's an
  example from the managed_accumulator stubs class:

[managed_accumulator_stubs_query_async]

* [*Synchronous]: To invoke an action in a fully synchronous manner, we can
  simply call [classref hpx::lcos::async]`<>().get()` (e.g., create a future and
  immediately wait on it to be ready). Here's an example from the
  managed_accumulator stubs class:

[managed_accumulator_stubs_add_sync]

[classref hpx::naming::id_type] is a type which represents a global identifier
in __hpx__. This is the type that is returned by [funcref hpx::find_here]`()`. This
type specifies the target of an action.

[heading The Client Class]

The following code is from
[hpx_link examples/accumulator/accumulators/managed_accumulator.hpp..managed_accumulator.hpp].

The client class is the primary interface to a component instance. Client classes
are used to create components:

``
    examples::managed_accumulator c;
    c.create(hpx::find_here()); // Create a component on this machine.
``

and to invoke component actions:

``
    c.add_sync(4);
``

Clients, like stubs and servers, need to inherit from a base class, this time,
[classref hpx::components::client_base]`<>`:

[import ../../examples/accumulator/accumulators/managed_accumulator.hpp]
[managed_accumulator_client_inherit]

For readability, we typedef the base class like so:

[managed_accumulator_base_type]

Here are examples of how to expose actions through a client class:

[managed_accumulator_client_reset_non_blocking]

[managed_accumulator_client_query_async]

[managed_accumulator_client_add_sync]

Note that `this->gid_` references a data member of the
[classref hpx::components::client_base]`<>` base class.

[endsect] [/Accumulator]

[/Proofread by:]
[/Phillip LeBlanc 3-13-12]

[/////////////////////////////////////////////////////////////////////////////]
[section:interest_calculator Interest Calculator]

__hpx__ provides its users with several different tools to simply express parallel
concepts. One of these tools is a local control object (LCO) called dataflow.
An LCO is a type of component that can spawn a new thread when triggered.
They are also distinguished from other components by a standard interface
which allow users to understand and use them easily.
Dataflows, being a LCO, is triggered when the values it depends
on become available. For instance, if you have a calculation X that depends on
the result of three other calculations, you could set up a dataflow that
would begin the calculation X as soon as the other three calculations
have returned their values.
Dataflows are set up to depend on other dataflows. It is this property
that makes dataflow a powerful parallelization tool. If you understand the
dependencies of your calculation, you can devise a simple algorithm which
sets up a dependency tree to be executed. In this example, we calculate
compound interest. To calculate compound interest, one must calculate
the interest made in each compound period, and then add that interest back to
the principal before calculating the interest made in the next period.
A practical person would of course use the formula for compound interest:

[teletype]

``
    F = P(1 + i) ^ n
    where:
        F= Future value
        P= Principal
        i= Interest rate
        n= number of compound periods
``

Nevertheless, we have chosen for the sake of example to manually calculate
the future value by iterating:

``
    I = P * i
     and
    P = P + I
``

[heading Setup]

The source code for this example can be found here:
[hpx_link examples/quickstart/interest_calculator.cpp..interest_calculator.cpp].

To compile this program, go to your __hpx__ build directory (see __getting_started__
for information on configuring and building __hpx__) and enter:

``
    $ make examples.quickstart.interest_calculator
``

To run the program type:

``
    $ ./bin/interest_calculator --principal 100 --rate 5 --cp 6 --time 36
``

This should print:

``
    Final amount: 134.01
    Amount made: 34.0096
``
[c++]

[heading Walkthrough]

Let us begin with main, here we can see that we again are using
__boost_program_options__ to set our command line variables (see
__fibonacci_example__ for more details). These options set the principal,
rate, compound period, and time. It is important to note that the units of time
for `cp` and `time` must be the same.

[import ../../examples/quickstart/interest_calculator.cpp]
[interest_main]

Next we look at hpx_main.

[interest_hpx_main]

Here we find our command line variables read in,
the rate is converted from a percent to a decimal, the number of calculation
iterations is determined, and then our dataflows are set up. Notice that we
first place our principal and rate into a dataflow by passing the variables
`p` and `i_rate` to an action called `identity_action`:

[interest_id_action]

In this way [classref hpx::lcos::dataflow_base]`<double> principal` and `rate`
will be initialized to `p` and `i_rate` when
[classref hpx::lcos::dataflow]`<identity_action>` returns a value. These
dataflows enter for loop and are passed to `interest`. Next `principal`
and `interest` are passed to the reassignment of `principal`. This loop
continues for each compound period that must be calculated. To see how
`interest` and `principal` are calculated in the loop let us look at
`calc_action` and `add_action`:

[interest_calc_add_action]

After the dataflow dependencies have been defined in hpx_main, we see the
following statement:

``
    double result = principal.get_future().get();
``

This statement calls [memberref hpx::lcos::future::get]`()` on the last dataflow
created by our for loop. The program will wait here until the entire dataflow
tree has been calculated and the value assigned to result. The program then prints
out the final value of the investment and the amount of interest made by subtracting
the final value of the investment from the initial value of the investment.

[endsect] [/Interest Calculator]

[/Proofread by:]


[/////////////////////////////////////////////////////////////////////////////]

[endsect] [/Examples]

