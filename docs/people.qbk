[/==============================================================================
    Copyright (C) 2007-2015 Hartmut Kaiser
    Copyright (C) 2016-2018 Adrian Serio

    Distributed under the Boost Software License, Version 1.0. (See accompanying
    file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
===============================================================================/]

[section People]

The __stellar__ Group (pronounced as stellar) stands for "[*S]ystems [*T]echnology,
[*E]mergent Para[*ll]elism, and [*A]lgorithm [*R]esearch". We are an
international group of faculty, researchers, and students working at various
institutions around the world.
The goal of the __stellar__ Group is to promote the development of
scalable parallel applications by providing a community for ideas, a framework
for collaboration, and a platform for communicating these concepts to the
broader community.

Our work is focused on building technologies for
scalable parallel applications. __hpx__, our general purpose C++ runtime system
for parallel and distributed applications, is no exception. We use __hpx__ for
a broad range of scientific applications, helping scientists and developers
to write code which scales better and shows better performance compared
to more conventional programming models such as MPI.

__hpx__ is based on /ParalleX/ which is a new
(and still experimental) parallel execution model aiming to
overcome the limitations imposed by the current hardware and the techniques
we use to write
applications today. Our group focuses on two types of applications - those
requiring excellent strong scaling, allowing for a dramatic reduction of
execution time for fixed workloads and those needing highest level of sustained
performance through massive parallelism. These applications are
presently unable (through conventional practices) to effectively exploit a
relatively small number of cores in a multi-core system. By extension,
these application will not be able to exploit high-end exascale
computing systems which are likely to employ hundreds of millions of such cores by the
end of this decade.

Critical bottlenecks to the effective use of new generation high performance
computing (HPC) systems include:

* /Starvation/: due to lack of usable application parallelism and means of
  managing it,
* /Overhead/: reduction to permit strong scalability, improve efficiency, and
  enable dynamic resource management,
* /Latency/: from remote access across system or to local memories,
* /Contention/: due to multicore chip I/O pins, memory banks, and system
  interconnects.

The ParalleX model has been devised to address these challenges by enabling a
new computing dynamic through the application of message-driven computation in
a global address space context with lightweight synchronization. The work on
__hpx__ is centered around implementing the concepts as defined by the ParalleX
model. __hpx__ is currently targeted at conventional machines, such as classical Linux based
Beowulf clusters and SMP nodes.

We fully understand that the success of __hpx__ (and ParalleX) is very much the
result of the work of many people. To see a list of who is contributing see
our tables below.

[heading HPX Contributors]

[table:contributors Contributors
  [[Name]                    [Institution]        [email]]
  [[Hartmut Kaiser]          [__cct__, __lsu__]   [[$./images/emails/hkaiser.png]]]
  [[Thomas Heller]           [__inf3__, __fau__]  [[$./images/emails/theller.png]]]
  [[Agustin Berge]           [__cct__, __lsu__]   [[$./images/emails/aberge.png]]]
  [[Mikael Simberg]          [__cscs__]           [[$./images/emails/msimberg.png]]]
  [[John Biddiscombe]        [__cscs__]           [[$./images/emails/biddisco.png]]]
  [[Anton Bikineev]          [__cct__, __lsu__]   [[$./images/emails/antbikineev.png]]]
  [[Martin Stumpf]           [__inf3__, __fau__]  [[$./images/emails/martinhstumpf.png]]]
  [[Bryce Adelstein Lelbach] [__nvidia__]         [[$./images/emails/blelbach.png]]]
  [[Shuangyang Yang]         [__cct__, __lsu__]   [[$./images/emails/syang16.png]]]
  [[Jeroen Habraken]         [__tue__]            [[$./images/emails/vexocide.png]]]
  [[Steven Brandt]           [__cct__, __lsu__]   [[$./images/emails/sbrandt.png]]]
  [[Antoine Tran Tan]        [__cct__, __lsu__]   [[$./images/emails/atrantan.png]]]
  [[Adrian Serio]            [__cct__, __lsu__]   [[$./images/emails/aserio.png]]]
  [[Maciej Brodowicz]        [__crest__, __iu__]  [[$./images/emails/mbrodowicz.png]]]
]

[heading Contributors to this Document]

[table:authors Documentation Authors
  [[Name]                    [Institution]        [email]]
  [[Hartmut Kaiser]          [__cct__, __lsu__]   [[$./images/emails/hkaiser.png]]]
  [[Thomas Heller]           [__inf3__, __fau__]  [[$./images/emails/theller.png]]]
  [[Bryce Adelstein Lelbach] [__nvidia__]         [[$./images/emails/blelbach.png]]]
  [[Vinay C Amatya]          [__cct__, __lsu__]   [[$./images/emails/vamatya.png]]]
  [[Steven Brandt]           [__cct__, __lsu__]   [[$./images/emails/sbrandt.png]]]
  [[Maciej Brodowicz]        [__crest__, __iu__]  [[$./images/emails/mbrodowicz.png]]]
  [[Adrian Serio]            [__cct__, __lsu__]   [[$./images/emails/aserio.png]]]
]

[heading Acknowledgements]

Thanks also to the following people who contributed directly or indirectly to
the project through discussions, pull requests, documentation patches, etc.

* Mikael Simberg (__cscs__), for his tireless help cleaning up 
  and maintaining __hpx__.
* Christopher Ogle, for his contributions to the parallel algorithms.
* Surya Priy, for his work with statistic performance counters.
* Anushi Maheshwari, for her work on random number generation.
* Bruno Pitrus, for his work with parallel algorithms.
* Nikunj Gupta, for his fixes for tests.
* Christopher Taylor, for his interest in __hpx__ and the fixes he provided.
* Shoshana Jakobovits, for her work on the resource partitioner.
* Denis Blank, who re-wrote our unwrapped function to accept plain values
  arbitrary containers, and properly deal with nested futures.
* Ajai V. George, who implemented several of the parallel algorithms.
* Taeguk Kwon, who worked on implementing parallel algorithms as well as 
  adapting the parallel algorithms to the Ranges TS.
* Zach Byerly (__lsu__), who in his work developing applications 
  on top of __hpx__
  opened tickets and contributed to the __hpx__ examples.
* Daniel Estermann, for his work porting __hpx__ to the Raspberry Pi.
* Alireza Kheirkhahan (__lsu__), who built and administered our 
  local cluster as well
  as his work in distributed IO.
* Abhimanyu Rawat, who worked on stack overflow detection.
* David Pfander, who improved signal handling in __hpx__, provided his
  optimization expertise, and worked on incorporating the Vc 
  vectorization into __hpx__.
* Denis Demidov, who contributed his insights with VexCL.
* Khalid Hasanov, who contributed changes which allowed to run __hpx__ on 64Bit
  power-pc architectures.
* Zahra Khatami (__lsu__), who contributed the prefetching iterators 
  and the persistent auto chunking executor parameters implementation.
* Marcin Copik, who worked on implementing GPU support for C++AMP and HCC.
  He also worked on implementing a HCC backend for __hpx_compute__.
* Minh-Khanh Do, who contributed the implementation of several segmented
  algorithms.
* Bibek Wagle (__lsu__), who worked on fixing and analyzing the 
  performance of the parcel coalescing plugin in __hpx__.
* Lukas Troska, who reported several problems and contributed various test cases
  allowing to reproduce the corresponding issues.
* Andreas Schaefer, who worked on integrating his library (__lgd__) with __hpx__.
  He reported various problems and submitted several patches to fix issues
  allowing for a better integration with __lgd__.
* Satyaki Upadhyay, who contributed several examples to __hpx__.
* Brandon Cordes, who contributed several improvements to the inspect tool.
* Harris Brakmic, who contributed an extensive build system description for
  building __hpx__ with Visual Studio.
* Parsa Amini (__lsu__), who refactored and simplified the implementation 
  of AGAS in __hpx__ and who works on its implementation and optimization.
* Luis Martinez de Bartolome who implemented a build system extension for
  __hpx__ integrating it with the __conan__ C/C++ package manager.
* Vinay C Amatya (__lsu__), who contributed to the documentation and 
  provided some of the __hpx__ examples.
* Kevin Huck and Nick Chaimov (__ou__), who contributed the integration of APEX
  (Autonomic Performance Environment for eXascale) with __hpx__.
* Francisco Jose Tapia, who helped with implementing the parallel sort
  algorithm for __hpx__.
* Patrick Diehl, who worked on implementing CUDA support for our companion
  library targeting GPGPUs (__hpxcl__).
* Eric Lemanissier contributed fixes to allow compilation using the MingW
  toolchain.
* Nidhi Makhijani who helped cleaning up some enum consistencies in __hpx__ and
  contributed to the resource manager used in the thread scheduling subsystem.
  She also worked on __hpx__ in the context of the Google Summer of Code 2015.
* Larry Xiao, Devang Bacharwar, Marcin Copik, and Konstantin Kronfeldner who
  worked on __hpx__ in the context of the Google Summer of Code program 2015.
* Daniel Bourgeois (__cct__) who contributed to __hpx__ the implementation of
  several parallel algorithms (as proposed by __cpp11_n4107__).
* Anuj Sharma and Christopher Bross (__inf3__), who worked on __hpx__ in the
  context of the __gsoc__ program 2014.
* Martin Stumpf (__inf3__), who rebuilt our contiguous testing infrastructure
  (see the __stellar_hpx_buildbot__). Martin is also working on __hpxcl__
  (mainly all work related to __opencl__) and implementing an __hpx__ backend
  for __pocl__, a portable computing language solution based on __opencl__.
* Grant Mercer (__unlv__), who helped creating many of the parallel algorithms
  (as proposed by __cpp11_n4107__).
* Damond Howard (__lsu__), who works on __hpxcl__ (mainly all work related to
  __cuda__).
* Christoph Junghans (Los Alamos National Lab), who helped making our
  buildsystem more portable.
* Antoine Tran Tan (Laboratoire de Recherche en Informatique, Paris), who
  worked on integrating __hpx__ as a backend for __nt2__. He also contributed
  an implementation of an API similar to Fortran co-arrays on top of __hpx__.
* John Biddiscombe (__cscs__), who helped with the BlueGene/Q port of
  __hpx__, implemented the parallel sort algorithm, and made several other
  contributions.
* Erik Schnetter (Perimeter Institute for Theoretical Physics), who greatly
  helped to make __hpx__ more robust by submitting a large amount of problem
  reports, feature requests, and made several direct contributions.
* Mathias Gaunard (Metascale), who contributed several patches to reduce
  compile time warnings generated while compiling __hpx__.
* Andreas Buhr, who helped with improving our documentation, especially by
  suggesting some fixes for inconsistencies.
* Patricia Grubel (__nmsu__), who contributed the description
  of the different __hpx__ thread scheduler policies and is working on the
  performance analysis of our thread scheduling subsystem.
* Lars Viklund, whose wit, passion for testing, and love of odd architectures 
  has been an amazing contribution to our team. He has also 
  contributed platform specific 
  patches for FreeBSD and MSVC12.
* Agustin Berge, who contributed patches fixing some very nasty hidden template
  meta-programming issues. He rewrote large parts of the API elements ensuring
  strict conformance with C++11/14.
* Anton Bikineev for contributing changes to make using `boost::lexical_cast`
  safer, he also contributed a thread safety fix to the iostreams module. He
  also contributed a complete rewrite of the serialization infrastructure
  replacing Boost.Serialization inside __hpx__.
* Pyry Jahkola, who contributed the Mac OS build system and build documentation
  on how to build __hpx__ using Clang and libc++.
* Mario Mulansky, who created an __hpx__ backend for his Boost.Odeint library,
  and who submitted several test cases allowing us to reproduce and fix problems
  in __hpx__.
* Rekha Raj, who contributed changes to the description of the Windows build
  instructions.
* Jeremy Kemp how worked on an __hpx__ OpenMP backend and added regression tests.
* Alex Nagelberg for his work on implementing a C wrapper API for __hpx__.
* Chen Guo, helvihartmann, Nicholas Pezolano, and John West who added
  and improved examples in __hpx__.
* Joseph Kleinhenz, Markus Elfring, Kirill Kropivyansky, Alexander Neundorf, 
  Bryant Lam, and Alex Hirsch who improved our CMake. 
* Jakub Golinowski, Praveen Velliengiri, Jean-Loup Tastet, Michael Levine, 
  Aalekh Nigam, HadrienG2, Prayag Verma, and
  Avyav Kumar who improved the documentation.
* J. F. Bastien, Christopher Hinz, Brandon Kohn, Mario Lang, 
  pierrele, hendrx, Dekken, 
  woodmeister123, Andrew Kemp, Dylan Stark, and Matthew Anderson who 
  contributed to the general improvement of __hpx__

In addition to the people who worked directly with __hpx__ development we would
like to acknowledge the NSF, DoE, DARPA, __cct__, __inf3__, and 
__cscs__ who fund and
support our work. We would also like to thank the following organizations for
granting us allocations of their compute resources:
LSU HPC, LONI, XSEDE, NERSC, and the Gauss Center for Supercomputing.

__hpx__ is currently funded by the following grants:

* The National Science Foundation through awards
  1117470 (APX), 1240655 (STAR), 1447831 (PXFS), 1339782 (STORM), and 
  1737785 (Phylanx).
  Any opinions, findings, and conclusions or
  recommendations expressed in this material are those of the author(s) and do
  not necessarily reflect the views of the National Science Foundation.
* The Department of Energy (DoE) through
  the award DE-SC0008714 (XPRESS). Neither the United States Government nor any
  agency thereof, nor any of their employees, makes any warranty, express or
  implied, or assumes any legal liability or responsibility for the accuracy,
  completeness, or usefulness of any information, apparatus, product, or process
  disclosed, or represents that its use would not infringe privately owned
  rights. Reference herein to any specific commercial product, process, or
  service by trade name, trademark, manufacturer, or otherwise does not
  necessarily constitute or imply its endorsement, recommendation, or favoring
  by the United States Government or any agency thereof. The views and opinions
  of authors expressed herein do not necessarily state or reflect those of the
  United States Government or any agency thereof.
* The Bavarian Research Foundation (Bayerische Forschungsstfitung)
  through the grant AZ-987-11.
* The European Commission's Horizon 2020 programme through the grant
  H2020-EU.1.2.2. 671603 (AllScale). 

[endsect] [/ People]

[/Proofread by:]
[/Adrian Serio 6-28-16]
[/Phillip LeBlanc 3-13-12]
