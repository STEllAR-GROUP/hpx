[/==============================================================================
    Copyright (C) 2007-2015 Hartmut Kaiser
    Copyright (C) 2016 Adrian Serio

    Distributed under the Boost Software License, Version 1.0. (See accompanying
    file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
===============================================================================/]

[section People]

The __stellar__ Group (pronounced as stellar) stands for "[*S]ystems [*T]echnology,
[*E]mergent Para[*ll]elism, and [*A]lgorithm [*R]esearch". We are an
international group of faculty, researchers, and students working at various
institutions around the world. 
The goal of the __stellar__ Group is to promote the development of
scalable parallel applications by providing a community for ideas, a framework
for collaboration, and a platform for communicating these concepts to the
broader community.

Our work is focused on building technologies for
scalable parallel applications. __hpx__, our general purpose C++ runtime system
for parallel and distributed applications, is no exception. We use __hpx__ for
a broad range of scientific applications, helping scientists and developers
to write code which scales better and shows better performance compared
to more conventional programming models such as MPI.

__hpx__ is based on /ParalleX/ which is a new
(and still experimental) parallel execution model aiming to
overcome the limitations imposed by the current hardware and the techniques 
we use to write
applications today. Our group focuses on two types of applications - those
requiring excellent strong scaling, allowing for a dramatic reduction of
execution time for fixed workloads and those needing highest level of sustained
performance through massive parallelism. These applications are
presently unable (through conventional practices) to effectively exploit a
relatively small number of cores in a multi-core system. By extension,
these application will not be able to exploit high-end exascale
computing systems which are likely to employ hundreds of millions of such cores by the
end of this decade.

Critical bottlenecks to the effective use of new generation high performance
computing (HPC) systems include:

* /Starvation/: due to lack of usable application parallelism and means of
  managing it,
* /Overhead/: reduction to permit strong scalability, improve efficiency, and
  enable dynamic resource management,
* /Latency/: from remote access across system or to local memories,
* /Contention/: due to multicore chip I/O pins, memory banks, and system
  interconnects.

The ParalleX model has been devised to address these challenges by enabling a
new computing dynamic through the application of message-driven computation in
a global address space context with lightweight synchronization. The work on
__hpx__ is centered around implementing the concepts as defined by the ParalleX
model. __hpx__ is currently targeted at conventional machines, such as classical Linux based
Beowulf clusters and SMP nodes.

We fully understand that the success of __hpx__ (and ParalleX) is very much the
result of the work of many people. To see a list of who is contributing see
our tables below.

[heading HPX Contributors]

[table:contributors Contributors
  [[Name]                    [Institution]        [email]]
  [[Hartmut Kaiser]          [__cct__, __lsu__]   [[$./images/emails/hkaiser.png]]]
  [[Thomas Heller]           [__inf3__, __fau__]  [[$./images/emails/theller.png]]]
  [[Agustin Berge]           [__cct__, __lsu__]   [[$./images/emails/aberge.png]]]
  [[John Biddiscombe]        [__cscs__]           [[$./images/emails/biddisco.png]]]
  [[Anton Bikineev]          [__cct__, __lsu__]   [[$./images/emails/antbikineev.png]]]
  [[Martin Stumpf]           [__inf3__, __fau__]  [[$./images/emails/martinhstumpf.png]]]
  [[Bryce Adelstein-Lelbach] [__llbl__]           [[$./images/emails/balelbach.png]]]
  [[Shuangyang Yang]         [__cct__, __lsu__]   [[$./images/emails/syang16.png]]]
  [[Jeroen Habraken]         [__tue__]            [[$./images/emails/vexocide.png]]]
  [[Steven Brandt]           [__cct__, __lsu__]   [[$./images/emails/sbrandt.png]]]
  [[Andrew Kemp]             [__cct__, __lsu__]   [[$./images/emails/akemp.png]]]
  [[Adrian Serio]            [__cct__, __lsu__]   [[$./images/emails/aserio.png]]]
  [[Maciej Brodowicz]        [__crest__, __iu__]  [[$./images/emails/mbrodowicz.png]]]
  [[Matthew Anderson]        [__crest__, __iu__]  [[$./images/emails/manderson.png]]]
  [[Dylan Stark]             [__sandia__]         [[$./images/emails/dstark.png]]]
]

[heading Contributors to this Document]

[table:authors Documentation Authors
  [[Name]                    [Institution]        [email]]
  [[Hartmut Kaiser]          [__cct__, __lsu__]   [[$./images/emails/hkaiser.png]]]
  [[Thomas Heller]           [__inf3__, __fau__]  [[$./images/emails/theller.png]]]
  [[Bryce Adelstein-Lelbach] [__cct__, __lsu__]   [[$./images/emails/blelbach.png]]]
  [[Vinay C Amatya]          [__cct__, __lsu__]   [[$./images/emails/vamatya.png]]]
  [[Steven Brandt]           [__cct__, __lsu__]   [[$./images/emails/sbrandt.png]]]
  [[Maciej Brodowicz]        [__crest__, __iu__]  [[$./images/emails/mbrodowicz.png]]]
  [[Adrian Serio]            [__cct__, __lsu__]   [[$./images/emails/aserio.png]]]
]

[heading Acknowledgements]

Thanks also to the following people who contributed directly or indirectly to
the project through discussions, pull requests, documentation patches, etc.

* Zahra Khatami, who contributed the prefetching iterators and the persistent
  auto chunking executor parameters implementation.
* Marcin Copik, who worked on implementing GPU support for C++AMP and HCC.
  He also worked on implementing a HCC backend for __hpx_compute__.
* Minh-Khanh Do, who contributed the implementation of several segmented
  algorithms.
* Bibek Wagle, who worked on fixing and analyzing the performance of the parcel
  coalescing plugin in __hpx__.
* Lukas Troska, who reported several problems and contributed various test cases
  allowing to reproduce the corresponding issues.
* Andreas Schaefer, who worked on integrating his library (__lgd__) with __hpx__.
  He reported various problems and submitted several patches to fix issues
  allowing for a better integration with __lgd__.
* Satyaki Upadhyay, who contributed several examples to __hpx__.
* Brandon Cordes, who contributed several improvements to the inspect tool.
* Harris Brakmic, who contributed an extensive build system description for
  building __hpx__ with Visual Studio.
* Parsa Amini, who refactored and simplified the implementation of AGAS in
  __hpx__.
* Luis Martinez de Bartolome who implemented a build system extension for
  __hpx__ integrating it with the __conan__ C/C++ package manager.
* Vinay C Amatya, who contributed to the documentation and provided some
  of the __hpx__ examples.
* Kevin Huck and Nick Chaimov (__ou__), who contributed the integration of APEX
  (Autonomic Performance Environment for eXascale) with __hpx__.
* Francisco Jose Tapia, who helped with implementing the parallel sort
  algorithm for __hpx__.
* Patrick Diehl, who worked on implementing CUDA support for our companion
  library targeting GPGPUs (__hpxcl__).
* Eric Lemanissier contributed fixes to allow compilation using the MingW
  toolchain.
* Nidhi Makhijani who helped cleaning up some enum consistencies in __hpx__ and
  contributed to the resource manager used in the thread scheduling subsystem.
  She also worked on __hpx__ in the context of the Google Summer of Code 2015.
* Larry Xiao, Devang Bacharwar, Marcin Copik, and Konstantin Kronfeldner who
  worked on HPX in the context of the Google Summer of Code program 2015.
* Daniel Bourgeois (__cct__) who contributed to __hpx__ the implementation of
  several parallel algorithms (as proposed by __cpp11_n4107__).
* Anuj Sharma and Christopher Bross (__inf3__), who worked on __hpx__ in the
  context of the __gsoc__ program 2014.
* Martin Stumpf (__inf3__), who rebuilt our contiguous testing infrastructure
  (see the __stellar_hpx_buildbot__). Martin is also working on __hpxcl__
  (mainly all work related to __opencl__) and implementing an __hpx__ backend
  for __pocl__, a portable computing language solution based on __opencl__.
* Grant Mercer (__unlv__), who helped creating many of the parallel algorithms
  (as proposed by __cpp11_n4107__).
* Damond Howard (__lsu__), who works on __hpxcl__ (mainly all work related to
  __cuda__).
* Parsa Amini (__cct__), who works on the implementation and optimization of
  AGAS (Active Global Address Space).
* Christoph Junghans (Los Alamos National Lab), who helped making our
  buildsystem more portable.
* Andreas Buhr, who helped with improving our documentation.
* Antoine Tran Tan (Laboratoire de Recherche en Informatique, Paris), who
  worked on integrating __hpx__ as a backend for __nt2__. He also contributed
  an implementation of an API similar to Fortran co-arrays on top of __hpx__.
* John Biddiscombe (__cscs__), who helped with the BlueGene/Q port of
  __hpx__, implemented the parallel sort algorithm, and made several other
  contributions.
* Erik Schnetter (Perimeter Institute for Theoretical Physics), who greatly
  helped to make __hpx__ more robust by submitting a large amount of problem
  reports, feature requests, and made several direct contributions.
* Mathias Gaunard (Metascale), who contributed several patches to reduce
  compile time warnings generated while compiling __hpx__.
* Andreas Buhr, who helped with improving our documentation, especially by
  suggesting some fixes for inconsistencies.
* Patricia Grubel (__nmsu__), who contributed the description
  of the different __hpx__ thread scheduler policies and is working on the
  performance analysis of our thread scheduling subsystem.
* Lars Viklund, who contributed platform specific patches for FreeBSD and MSVC12.
* Agustin Berge, who contributed patches fixing some very nasty hidden template
  meta-programming issues. He rewrote large parts of the API elements ensuring
  strict conformance with C++11/14.
* Anton Bikineev for contributing changes to make using `boost::lexical_cast`
  safer, he also contributed a thread safety fix to the iostreams module. He
  also contributed a complete rewrite of the serialization infrastructure
  replacing Boost.Serialization inside __hpx__.
* Pyry Jahkola, who contributed the Mac OS build system and build documentation
  on how to build HPX using Clang and libc++.
* Mario Mulansky, who created an __hpx__ backend for his Boost.Odeint library, and
  who submitted several test cases allowing us to reproduce and fix problems
  in __hpx__.
* Rekha Raj, who contributed changes to the description of the Windows build
  instructions.
* Alex Nagelberg for his work on implementing a C wrapper API for HPX.

In addition to the people who worked directly with __hpx__ development we would
like to acknowledge the NSF, DoE, DARPA, __cct__, and __inf3__ who fund and
support our work. We would also like to thank the following organizations for
granting us allocations of their compute resources:
LSU HPC, LONI, XSEDE and the Gauss Center for Supercomputing.

__hpx__ is currently funded by the following grants:

* The National Science Foundation through awards
  1117470 (APX), 1240655 (STAR), 1447831 (PXFS), and 1339782 (STORM).
  Any opinions, findings, and conclusions or
  recommendations expressed in this material are those of the author(s) and do
  not necessarily reflect the views of the National Science Foundation.
* The Department of Energy (DoE) through
  the award DE-SC0008714 (XPRESS). Neither the United States Government nor any
  agency thereof, nor any of their employees, makes any warranty, express or
  implied, or assumes any legal liability or responsibility for the accuracy,
  completeness, or usefulness of any information, apparatus, product, or process
  disclosed, or represents that its use would not infringe privately owned
  rights. Reference herein to any specific commercial product, process, or
  service by trade name, trademark, manufacturer, or otherwise does not
  necessarily constitute or imply its endorsement, recommendation, or favoring
  by the United States Government or any agency thereof. The views and opinions
  of authors expressed herein do not necessarily state or reflect those of the
  United States Government or any agency thereof.
* The Bavarian Research Foundation (Bayerische Forschungsstfitung)
  through the grant AZ-987-11.


[endsect] [/ People]

[/Proofread by:]
[/Adrian Serio 6-28-16]
[/Phillip LeBlanc 3-13-12]
