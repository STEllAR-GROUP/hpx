{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 5: GPU Acceleration with HPXPy\n",
    "\n",
    "This tutorial covers GPU acceleration in HPXPy using HPX's executor infrastructure:\n",
    "\n",
    "1. Checking GPU availability (CUDA and SYCL)\n",
    "2. Transparent device selection (recommended)\n",
    "3. Creating arrays on different devices\n",
    "4. Data transfers between CPU and GPU\n",
    "5. SYCL support for cross-platform GPUs\n",
    "6. Async GPU operations with HPX futures\n",
    "\n",
    "HPXPy supports multiple GPU backends through HPX's executor infrastructure:\n",
    "- **CUDA**: For NVIDIA GPUs via `hpx::cuda::experimental::cuda_executor`\n",
    "- **SYCL**: For Intel, AMD, Apple Silicon GPUs via `hpx::sycl::experimental::sycl_executor`\n",
    "\n",
    "Both backends use HPX for:\n",
    "- Async operations with proper future integration\n",
    "- GIL release during GPU operations\n",
    "- Consistent API across platforms\n",
    "\n",
    "HPXPy provides two ways to work with GPUs:\n",
    "- **Transparent API**: Use the `device` parameter on regular functions (recommended)\n",
    "- **Explicit API**: Use `hpx.gpu` (CUDA) or `hpx.sycl` modules directly\n",
    "\n",
    "The transparent API is preferred because it makes your code portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hpxpy as hpx\n",
    "\n",
    "# Initialize HPX runtime\n",
    "hpx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Checking GPU Availability\n",
    "\n",
    "Before using GPU features, check what's available on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPU backends are available\n",
    "print(\"=== GPU Backend Availability ===\")\n",
    "print(f\"CUDA available: {hpx.gpu.is_available()}\")\n",
    "print(f\"SYCL available: {hpx.sycl.is_available()}\")\n",
    "\n",
    "if hpx.gpu.is_available():\n",
    "    print(f\"  CUDA devices: {hpx.gpu.device_count()}\")\n",
    "    \n",
    "if hpx.sycl.is_available():\n",
    "    print(f\"  SYCL devices: {hpx.sycl.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available CUDA devices\n",
    "cuda_devices = hpx.gpu.get_devices()\n",
    "\n",
    "if cuda_devices:\n",
    "    print(\"Available CUDA GPUs:\")\n",
    "    for dev in cuda_devices:\n",
    "        print(f\"  [{dev.id}] {dev.name}\")\n",
    "        print(f\"      Memory: {dev.total_memory_gb():.1f} GB\")\n",
    "        print(f\"      Compute Capability: {dev.compute_capability()}\")\n",
    "        print(f\"      Multiprocessors: {dev.multiprocessor_count}\")\n",
    "else:\n",
    "    print(\"No CUDA GPUs available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available SYCL devices\n",
    "sycl_devices = hpx.sycl.get_devices()\n",
    "\n",
    "if sycl_devices:\n",
    "    print(\"Available SYCL GPUs:\")\n",
    "    for dev in sycl_devices:\n",
    "        print(f\"  [{dev.id}] {dev.name}\")\n",
    "        print(f\"      Vendor: {dev.vendor}\")\n",
    "        print(f\"      Backend: {dev.backend}\")\n",
    "        print(f\"      Memory: {dev.global_mem_size_gb():.1f} GB\")\n",
    "        print(f\"      Compute Units: {dev.max_compute_units}\")\n",
    "else:\n",
    "    print(\"No SYCL GPUs available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Transparent Device Selection (Recommended)\n",
    "\n",
    "The recommended way to use GPUs in HPXPy is through the `device` parameter on array creation functions. This makes your code portable and easy to read.\n",
    "\n",
    "### Device Options\n",
    "\n",
    "| Value | Behavior |\n",
    "|-------|----------|\n",
    "| `None` or `'cpu'` | Create array on CPU (default) |\n",
    "| `'gpu'` or `'cuda'` | Create array on CUDA GPU (error if unavailable) |\n",
    "| `'sycl'` | Create array on SYCL GPU (Intel/AMD/Apple) |\n",
    "| `'auto'` | Use best available: CUDA > SYCL > CPU |\n",
    "| `0`, `1`, etc. | Use specific GPU device ID |\n",
    "\n",
    "The `'auto'` option is especially useful - it automatically selects the best available backend:\n",
    "- On systems with NVIDIA GPUs: uses CUDA\n",
    "- On systems with SYCL GPUs: uses SYCL\n",
    "- Otherwise: falls back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays on CPU (default behavior)\n",
    "cpu_arr = hpx.zeros(1000)\n",
    "print(f\"CPU array shape: {cpu_arr.shape}\")\n",
    "\n",
    "# Explicit CPU\n",
    "cpu_arr2 = hpx.zeros(1000, device='cpu')\n",
    "print(f\"Explicit CPU array shape: {cpu_arr2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'auto' for portable code - best GPU if available, CPU otherwise\n",
    "# This is the recommended approach for most use cases\n",
    "\n",
    "arr = hpx.zeros(10000, device='auto')\n",
    "print(f\"Array created with device='auto'\")\n",
    "print(f\"  Shape: {arr.shape}\")\n",
    "\n",
    "# Check where the array lives\n",
    "if hasattr(arr, 'device'):\n",
    "    print(f\"  Location: GPU device {arr.device}\")\n",
    "else:\n",
    "    print(f\"  Location: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All array creation functions support the device parameter\n",
    "\n",
    "# zeros, ones, empty\n",
    "z = hpx.zeros((100, 100), device='auto')\n",
    "o = hpx.ones((100, 100), device='auto')\n",
    "e = hpx.empty(1000, device='auto')\n",
    "\n",
    "# full - create array filled with a value\n",
    "f = hpx.full(1000, 3.14159, device='auto')\n",
    "\n",
    "# arange - evenly spaced values\n",
    "a = hpx.arange(10000, device='auto')\n",
    "\n",
    "# linspace - evenly spaced over interval\n",
    "l = hpx.linspace(0, 1, 100, device='auto')\n",
    "\n",
    "print(\"Created arrays with device='auto':\")\n",
    "print(f\"  zeros: {z.shape}\")\n",
    "print(f\"  ones: {o.shape}\")\n",
    "print(f\"  empty: {e.shape}\")\n",
    "print(f\"  full: {f.shape}\")\n",
    "print(f\"  arange: {a.shape}\")\n",
    "print(f\"  linspace: {l.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer numpy arrays to the preferred device\n",
    "np_data = np.random.randn(1000).astype(np.float64)\n",
    "\n",
    "# Using from_numpy\n",
    "arr1 = hpx.from_numpy(np_data, device='auto')\n",
    "\n",
    "# Using array\n",
    "arr2 = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0], device='auto')\n",
    "\n",
    "print(f\"from_numpy result: {arr1.shape}\")\n",
    "print(f\"array result: {arr2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Data Transfers\n",
    "\n",
    "All HPXPy arrays (CPU and GPU) support the `to_numpy()` method to transfer data back to a NumPy array on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array (on GPU if available)\n",
    "arr = hpx.arange(10, device='auto')\n",
    "\n",
    "# Transfer back to CPU as NumPy array\n",
    "np_result = arr.to_numpy()\n",
    "\n",
    "print(f\"Original HPXPy array: {arr}\")\n",
    "print(f\"NumPy result: {np_result}\")\n",
    "print(f\"NumPy type: {type(np_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip: NumPy -> HPXPy (auto device) -> NumPy\n",
    "original = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(f\"Original NumPy: {original}\")\n",
    "\n",
    "# Transfer to best available device\n",
    "hpx_arr = hpx.from_numpy(original, device='auto')\n",
    "print(f\"HPXPy array created\")\n",
    "\n",
    "# Transfer back\n",
    "result = hpx_arr.to_numpy()\n",
    "print(f\"Back to NumPy: {result}\")\n",
    "\n",
    "# Verify data integrity\n",
    "np.testing.assert_array_equal(original, result)\n",
    "print(\"Data integrity verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4. SYCL Support (Cross-Platform GPUs)\n",
    "\n",
    "HPXPy supports SYCL via `hpx::sycl::experimental::sycl_executor`. SYCL provides cross-platform GPU support for:\n",
    "\n",
    "| Backend | Platform | SYCL Implementation |\n",
    "|---------|----------|---------------------|\n",
    "| Level-Zero | Intel GPUs | Intel oneAPI |\n",
    "| HIP | AMD GPUs | AdaptiveCpp |\n",
    "| CUDA | NVIDIA GPUs | AdaptiveCpp, oneAPI |\n",
    "| Metal | Apple Silicon | AdaptiveCpp (experimental) |\n",
    "| OpenCL | Various | Intel oneAPI, AdaptiveCpp |\n",
    "\n",
    "### Using SYCL Explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hpx.sycl.is_available():\n",
    "    print(f\"SYCL available with {hpx.sycl.device_count()} device(s)\")\n",
    "    \n",
    "    # Get device info\n",
    "    dev = hpx.sycl.get_device(0)\n",
    "    print(f\"Device: {dev.name}\")\n",
    "    print(f\"Backend: {dev.backend}\")\n",
    "    print(f\"Memory: {dev.global_mem_size_gb():.1f} GB\")\n",
    "    \n",
    "    # Create arrays explicitly on SYCL\n",
    "    sycl_arr = hpx.zeros(1000, device='sycl')\n",
    "    print(f\"\\nCreated SYCL array: {sycl_arr.shape}, device={sycl_arr.device}\")\n",
    "    \n",
    "    # All creation functions work with SYCL\n",
    "    z = hpx.sycl.zeros([100, 100])\n",
    "    o = hpx.sycl.ones([1000])\n",
    "    f = hpx.sycl.full([100], 3.14)\n",
    "    \n",
    "    print(f\"\\nCreated various SYCL arrays:\")\n",
    "    print(f\"  zeros: {z.shape}\")\n",
    "    print(f\"  ones: {o.shape}\")\n",
    "    print(f\"  full: {f.shape}\")\n",
    "    \n",
    "    # Transfer data\n",
    "    np_data = np.random.randn(100)\n",
    "    sycl_from_np = hpx.sycl.from_numpy(np_data)\n",
    "    back_to_np = sycl_from_np.to_numpy()\n",
    "    np.testing.assert_array_almost_equal(np_data, back_to_np)\n",
    "    print(\"\\nNumPy <-> SYCL round-trip verified!\")\n",
    "    \n",
    "    # SYCL operations\n",
    "    ones = hpx.sycl.ones([100])\n",
    "    total = hpx.sycl.sum(ones)\n",
    "    print(f\"Sum of 100 ones: {total}\")\n",
    "else:\n",
    "    print(\"SYCL not available\")\n",
    "    print(\"Build HPX with HPX_WITH_SYCL=ON and HPXPy with HPXPY_WITH_SYCL=ON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Explicit CUDA GPU API\n",
    "\n",
    "For more control over NVIDIA GPUs, you can use the `hpx.gpu` module directly. This uses `hpx::cuda::experimental::cuda_executor` for proper HPX integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hpx.gpu.is_available():\n",
    "    # Get current GPU\n",
    "    current = hpx.gpu.current_device()\n",
    "    print(f\"Current GPU: {current}\")\n",
    "    \n",
    "    # Get memory info\n",
    "    free, total = hpx.gpu.memory_info(current)\n",
    "    print(f\"GPU Memory: {free / 1e9:.2f} GB free / {total / 1e9:.2f} GB total\")\n",
    "    \n",
    "    # Create array explicitly on GPU\n",
    "    gpu_arr = hpx.gpu.zeros([1000, 1000])\n",
    "    print(f\"Created GPU array: {gpu_arr.shape} on device {gpu_arr.device}\")\n",
    "    \n",
    "    # Synchronize GPU (wait for all operations to complete)\n",
    "    hpx.gpu.synchronize()\n",
    "    print(\"GPU synchronized\")\n",
    "else:\n",
    "    print(\"CUDA not available - skipping explicit GPU examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hpx.gpu.is_available():\n",
    "    # Explicit GPU array operations\n",
    "    \n",
    "    # Create arrays\n",
    "    zeros = hpx.gpu.zeros([100])\n",
    "    ones = hpx.gpu.ones([100])\n",
    "    full = hpx.gpu.full([100], 42.0)\n",
    "    \n",
    "    # Fill with value\n",
    "    zeros.fill(3.14)\n",
    "    \n",
    "    # Transfer from numpy\n",
    "    np_data = np.random.randn(100)\n",
    "    from_np = hpx.gpu.from_numpy(np_data)\n",
    "    \n",
    "    # GPU sum\n",
    "    total = hpx.gpu.sum(ones)\n",
    "    print(f\"Sum of ones: {total}\")\n",
    "    \n",
    "    # Transfer back to CPU\n",
    "    result = full.to_numpy()\n",
    "    print(f\"Full array (first 5): {result[:5]}\")\n",
    "else:\n",
    "    print(\"CUDA not available - skipping explicit GPU examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Async GPU Operations\n",
    "\n",
    "HPXPy supports async GPU operations that return HPX futures. This allows overlapping GPU transfers with other computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hpx.gpu.is_available():\n",
    "    # Enable async operations (starts HPX CUDA polling)\n",
    "    hpx.gpu.enable_async()\n",
    "    \n",
    "    # Create GPU array\n",
    "    arr = hpx.gpu.zeros([1000000])\n",
    "    data = np.random.rand(1000000)\n",
    "    \n",
    "    # Async copy - returns immediately\n",
    "    future = arr.async_from_numpy(data)\n",
    "    print(f\"Async copy started, future.is_ready(): {future.is_ready()}\")\n",
    "    \n",
    "    # Do other work while transfer happens...\n",
    "    print(\"Doing other work while transfer in progress...\")\n",
    "    \n",
    "    # Wait for completion\n",
    "    future.get()\n",
    "    print(\"Transfer complete!\")\n",
    "    \n",
    "    # Verify data\n",
    "    result = arr.to_numpy()\n",
    "    np.testing.assert_array_almost_equal(data, result)\n",
    "    print(\"Data verified!\")\n",
    "    \n",
    "    # Disable async when done\n",
    "    hpx.gpu.disable_async()\n",
    "else:\n",
    "    print(\"CUDA not available - skipping async examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the AsyncContext context manager\n",
    "if hpx.gpu.is_available():\n",
    "    arr1 = hpx.gpu.zeros([100000])\n",
    "    arr2 = hpx.gpu.zeros([100000])\n",
    "    data1 = np.ones(100000)\n",
    "    data2 = np.ones(100000) * 2\n",
    "    \n",
    "    with hpx.gpu.AsyncContext():\n",
    "        # Multiple async transfers\n",
    "        f1 = arr1.async_from_numpy(data1)\n",
    "        f2 = arr2.async_from_numpy(data2)\n",
    "        \n",
    "        print(\"Both transfers started\")\n",
    "        \n",
    "        # Wait for both\n",
    "        f1.get()\n",
    "        f2.get()\n",
    "    \n",
    "    # Async automatically disabled when exiting context\n",
    "    print(\"Both transfers complete!\")\n",
    "    print(f\"arr1 sum: {hpx.gpu.sum(arr1)}\")\n",
    "    print(f\"arr2 sum: {hpx.gpu.sum(arr2)}\")\n",
    "else:\n",
    "    print(\"CUDA not available - skipping async context example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Writing Portable Code\n",
    "\n",
    "The `device='auto'` pattern makes it easy to write code that automatically uses the GPU when available, but gracefully falls back to CPU when not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_on_best_device(size):\n",
    "    \"\"\"\n",
    "    Example function that automatically uses the best available device.\n",
    "    \n",
    "    This code works identically on systems with or without GPUs.\n",
    "    \"\"\"\n",
    "    # Create data on best device\n",
    "    data = hpx.arange(size, device='auto')\n",
    "    \n",
    "    # Get the result back as NumPy\n",
    "    result = data.to_numpy()\n",
    "    \n",
    "    return result.sum()\n",
    "\n",
    "# This works on any system\n",
    "result = compute_on_best_device(10000)\n",
    "print(f\"Sum of 0 to 9999: {result}\")\n",
    "print(f\"Expected: {sum(range(10000))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: Check device at runtime for logging/debugging\n",
    "def create_with_info(shape, device='auto'):\n",
    "    \"\"\"Create array and report where it was created.\"\"\"\n",
    "    arr = hpx.zeros(shape, device=device)\n",
    "    \n",
    "    if hasattr(arr, 'device'):\n",
    "        location = f\"GPU {arr.device}\"\n",
    "    else:\n",
    "        location = \"CPU\"\n",
    "    \n",
    "    print(f\"Created {shape} array on {location}\")\n",
    "    return arr\n",
    "\n",
    "arr = create_with_info((1000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 8. Error Handling\n",
    "\n",
    "Understanding how HPXPy handles GPU-related errors helps write robust code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device='auto' never raises an error - it falls back to CPU\n",
    "arr = hpx.zeros(100, device='auto')  # Always works\n",
    "print(\"device='auto' always succeeds\")\n",
    "\n",
    "# device='gpu' raises RuntimeError if CUDA not available\n",
    "if not hpx.gpu.is_available():\n",
    "    try:\n",
    "        arr = hpx.zeros(100, device='gpu')\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Expected error: {e}\")\n",
    "\n",
    "# device='sycl' raises RuntimeError if SYCL not available\n",
    "if not hpx.sycl.is_available():\n",
    "    try:\n",
    "        arr = hpx.zeros(100, device='sycl')\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Expected error: {e}\")\n",
    "\n",
    "# Invalid device specification raises ValueError\n",
    "try:\n",
    "    arr = hpx.zeros(100, device='invalid')\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "1. **Use `device='auto'` for portable code** - Your code will run on any system (CUDA, SYCL, or CPU)\n",
    "\n",
    "2. **Use `device='gpu'` or `device='sycl'` when specific backend is required** - Get a clear error if unavailable\n",
    "\n",
    "3. **Check availability for conditional logic** - Use `hpx.gpu.is_available()` or `hpx.sycl.is_available()`\n",
    "\n",
    "4. **Minimize data transfers** - GPU-CPU transfers have overhead; keep data on device when possible\n",
    "\n",
    "5. **Use larger arrays for GPU benefit** - Small arrays may be faster on CPU due to overhead\n",
    "\n",
    "6. **Use async operations for overlap** - `async_from_numpy()` allows overlapping transfers with computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Device Selection Options\n",
    "\n",
    "| Device | Description |\n",
    "|--------|-------------|\n",
    "| `None` / `'cpu'` | CPU (NumPy) |\n",
    "| `'gpu'` / `'cuda'` | NVIDIA CUDA GPU |\n",
    "| `'sycl'` | SYCL GPU (Intel/AMD/Apple Silicon) |\n",
    "| `'auto'` | Best available (CUDA > SYCL > CPU) |\n",
    "| `0`, `1`, ... | Specific GPU device ID |\n",
    "\n",
    "### Backend Comparison\n",
    "\n",
    "| Feature | CUDA | SYCL |\n",
    "|---------|------|------|\n",
    "| HPX Executor | `cuda_executor` | `sycl_executor` |\n",
    "| Platforms | NVIDIA only | Intel, AMD, Apple, NVIDIA |\n",
    "| Async Support | Yes | Yes |\n",
    "| Future Integration | HPX futures | HPX futures |\n",
    "\n",
    "### API Comparison\n",
    "\n",
    "| Feature | Transparent API | CUDA Explicit | SYCL Explicit |\n",
    "|---------|----------------|---------------|---------------|\n",
    "| Create zeros | `hpx.zeros(shape, device='auto')` | `hpx.gpu.zeros(shape)` | `hpx.sycl.zeros(shape)` |\n",
    "| Create ones | `hpx.ones(shape, device='auto')` | `hpx.gpu.ones(shape)` | `hpx.sycl.ones(shape)` |\n",
    "| Create full | `hpx.full(shape, val, device='auto')` | `hpx.gpu.full(shape, val)` | `hpx.sycl.full(shape, val)` |\n",
    "| From numpy | `hpx.from_numpy(arr, device='auto')` | `hpx.gpu.from_numpy(arr)` | `hpx.sycl.from_numpy(arr)` |\n",
    "| To numpy | `arr.to_numpy()` | `arr.to_numpy()` | `arr.to_numpy()` |\n",
    "| Check available | - | `hpx.gpu.is_available()` | `hpx.sycl.is_available()` |\n",
    "| Portable? | Yes | No | No |\n",
    "\n",
    "**Recommendation**: Use the transparent API with `device='auto'` for portable code that runs on any system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Portable scientific computation\n",
    "\n",
    "def portable_computation(n):\n",
    "    \"\"\"\n",
    "    A computation that runs on GPU if available, CPU otherwise.\n",
    "    \"\"\"\n",
    "    # Create data on best device\n",
    "    x = hpx.linspace(0, 2 * np.pi, n, device='auto')\n",
    "    \n",
    "    # Get back to numpy for computation\n",
    "    # (In future versions, operations will run directly on GPU)\n",
    "    x_np = x.to_numpy()\n",
    "    result = np.sin(x_np)\n",
    "    \n",
    "    return result\n",
    "\n",
    "result = portable_computation(1000)\n",
    "print(f\"Computed sin(x) for 1000 points\")\n",
    "print(f\"Result range: [{result.min():.4f}, {result.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "hpx.finalize()\n",
    "print(\"HPX runtime finalized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
