#!/bin/bash

# This function writes a slurm script. 
# We can call it with different parameter 
# settings to create different experiments

function write_script
{
JOB_NAME=$(printf 'hpx-N%04d-T%05d-t%02d-%s' ${NODES} ${TRANSFERSIZE} ${THREADS_PERTASK} ${PARCELTYPE})
DIR_NAME=$(printf 'hpx-N%04d-T%05d-t%02d-%s' ${NODES} ${TRANSFERSIZE} ${THREADS_PERTASK} ${PARCELTYPE})
TASKS_PER_NODE=1

echo "Creating job $DIR_NAME"

mkdir -p $DIR_NAME

cat << _EOF_ > ${DIR_NAME}/submit-job.bash
#!/bin/bash

#SBATCH --job-name=HPX-Test
#SBATCH --output=slurm.out
#SBATCH --error=slurm.err
#SBATCH --partition=${QUEUE}
#SBATCH --nodes=${NODES}
#SBATCH --time=${TIME}
#SBATCH --dependency=singleton
## #SBATCH --cpus-per-task=1
#SBATCH --exclusive 
#SBATCH --distribution=cyclic

#======START=====
module load slurm

#
# mvapich settings used at CSCS
# not all are relevant for this test
#
export LD_LIBRARY_PATH=${LIB_PATH}:${LD_LIBRARY_PATH}
export HDF5_PARAPREFIX=${BASEDIR}/${DIR_NAME}
export BGLOCKLESSMPIO_F_TYPE=0x47504653
export MV2_ALLTOALL_MEDIUM_MSG=524288
export MpiDefault=none
export MV2_IBA_HCA=roq
export MV2_USE_RDMA_CM=1
export MV2_USE_IWARP_MODE=1
export MV2_ENABLE_AFFINITY=0

${MPIEXEC} -n $[${SERVERS_PERNODE} * $NODES] ${EXECUTABLE1} ${PROGRAM_PARAMS}

_EOF_

chmod 775 ${DIR_NAME}/submit-job.bash

echo "cd ${DIR_NAME}; sbatch submit-job.bash; cd \$BASEDIR" >> run_jobs.bash

}

# get the path to this generate script, works for most cases
pushd `dirname $0` > /dev/null
BASEDIR=`pwd`
popd > /dev/null
echo "Generating jobs using base directory $BASEDIR"

# Create another script to submit all generated jobs to the scheduler
echo "#!/bin/bash" > run_jobs.bash
echo "BASEDIR=$BASEDIR" >> run_jobs.bash
echo "cd $BASEDIR" >> run_jobs.bash
chmod 775 run_jobs.bash
echo "Use find . -name \*.out -exec grep CSVData {} \;"

#
# 
#
MPIEXEC="@MPIEXEC@"
QUEUE=nvp0
EXECUTABLE1=@EXE_PATH@
LIB_PATH="@LIB_PATH@"
JOB_OPTIONS1="@JOB_OPTIONS1@"
MEMPERNODE=
TIME="02:00:00"
CLIENTS_PERNODE=0
SERVERS_PERNODE=1

# Loop through all the parameter combinations generating jobs for each
# "tcp" "mpi" "ibverbs"
for NODES in 1 2 4 8 16
do
  for PARCELTYPE in "tcp" "ibverbs"
  do
    for TRANSFERSIZE in 1024 2048 4096 8192 16384
    do
      for THREADS_PERTASK in 2 4 8 16 32 50
      do
        LOCAL_SIZE=$(printf "%.0f" $( bc <<< "scale=6;(128 * $TRANSFERSIZE * $THREADS_PERTASK)/1024" ))
        LOCAL_SIZE=$(echo $((LOCAL_SIZE>512?512:LOCAL_SIZE)))
        PROGRAM_PARAMS="--hpx:run-hpx-main -Ihpx.parcel.${PARCELTYPE}.enable=1 --hpx:threads=${THREADS_PERTASK} --hpx:bind=balanced --localMB=${LOCAL_SIZE} --transferKB=${TRANSFERSIZE}"
        write_script
      done
    done
  done
done
